[{"id":0,"href":"/www6vCloudNative/docs/orchestrate/k8sDeployment/","title":"Kubernetes Deployment","section":"编排原理","content":"\nDeployment # Pod-template-hash label Rolling Update Deployment Max Surge Max Unavailable Argo Rollouts [5] # Argo Rollouts 是一个 Kubernetes 控制器，它提供了在应用程序部署过程中执行渐进式发布和蓝绿部署等高级部署策略的能力。它是基于 Kubernetes 原生的 Deployment 资源构建的，通过引入新的 Rollout 资源来扩展和增强部署控制。\n参考 # Deployment D\n如何在 Kubernetes 中对无状态应用进行分批发布 阿里 孙齐（代序） 第6 章 ： 应用编排与管理： Deployment 阿里 kubernetes 最佳实践：优雅热更新 陈鹏 Deployment jimmysong Argo Rollouts 中文文档 jimmysong "},{"id":1,"href":"/www6vCloudNative/docs/orchestrate/k8sPLEG/","title":"kubelet和PLEG","section":"编排原理","content":"\nkubelet管理Pod的核心流程 # Pod启动流程 [2][3] # PLEG # relist操作\n参考：\nkubelet 原理解析二：pleg 《模块七：Kubernetes控制平面组件》 从架构层面了解Kubernetes "},{"id":2,"href":"/www6vCloudNative/ref/cloudNative/k8sAdmissionWebhook/","title":"K8s  AdmissionWebhook","section":"Refs","content":"\nMutatingAdmissionWebhook 是如何工作的[1][chat] # MutatingAdmissionWebhook拦截与MutatingWebhookConfiguration中定义的规则匹配的请求，然后将其发送到Webhook服务器进行处理，然后再持久化到etcd 中。 MutatingAdmissionWebhook通过向Webhook服务器发送admission请求来执行变更。Webhook服务器只是遵循Kubernetes的API 的普通HTTP服务器。\n以下图示了MutatingAdmissionWebhook的工作原理:\nMutatingAdmissionWebhook需要三个对象才能正常工作:\nMutatingWebhookConfiguration\nMutatingAdmissionWebhook需要在apiserver中注册，提供MutatingWebhookConfiguration。在注册过程中，MutatingAdmissionWebhook说明以下内容:\n如何连接到Webhook Admission服务器 如何验证Webhook Admission服务器 Webhook Admission服务器的URL路径 定义了哪些资源和操作它处理的规则 如何处理来自Webhook Admission服务器的无法识别的错误 apiVersion: admissionregistration.k8s.io/v1beta1\rkind: MutatingWebhookConfiguration\rmetadata:\rname: sidecar-injector-webhook-cfg\rlabels:\rapp: sidecar-injector\rwebhooks:\r- name: sidecar-injector.morven.me\rclientConfig:\rservice:\rname: sidecar-injector-webhook-svc #2\rnamespace: default\rpath: \u0026#34;/mutate\u0026#34;\rcaBundle: ${CA_BUNDLE}\rrules:\r- operations: [ \u0026#34;CREATE\u0026#34; ]\rapiGroups: [\u0026#34;\u0026#34;]\rapiVersions: [\u0026#34;v1\u0026#34;]\rresources: [\u0026#34;pods\u0026#34;]\rnamespaceSelector:\rmatchLabels:\rsidecar-injector: enabled MutatingAdmissionWebhook本身\nMutatingAdmissionWebhook是一种插件式的Admission控制器，可以配置到apiserver中。MutatingAdmissionWebhook插件从MutatingWebhookConfiguration中获取感兴趣的Admission Webhooks列表。然后，MutatingAdmissionWebhook观察到对apiserver的请求，并拦截与admission webhook规则匹配的请求，并并行地调用它们。\nWebhook Admission Server\nWebhook Admission服务器只是一个符合Kubernetes API的普通HTTP服务器。对于每个API server的请求，MutatingAdmissionWebhook将admissionReview（用于参考的API）发送到相关的webhook admission服务器。webhook admission服务器会从admissionReview中收集信息，如object，oldobject和userInfo，然后返回一个admissionReview响应，其中包括填充了admission决策和可选的Patch以改变资源的AdmissionResponse。\n服务部署 # apiVersion: extensions/v1beta1\rkind: Deployment\rmetadata:\rname: sidecar-injector-webhook-deployment\rlabels:\rapp: sidecar-injector\rspec:\rreplicas: 1\rtemplate:\rmetadata:\rlabels:\rapp: sidecar-injector\rspec:\rcontainers:\r- name: sidecar-injector\rimage: morvencao/sidecar-injector:v1\rimagePullPolicy: IfNotPresent\rargs:\r- -sidecarCfgFile=/etc/webhook/config/sidecarconfig.yaml #1\r- -tlsCertFile=/etc/webhook/certs/cert.pem\r- -tlsKeyFile=/etc/webhook/certs/key.pem\r- -alsologtostderr\r- -v=4\r- 2\u0026gt;\u0026amp;1\rvolumeMounts:\r- name: webhook-certs\rmountPath: /etc/webhook/certs\rreadOnly: true\r- name: webhook-config\rmountPath: /etc/webhook/config\rvolumes:\r- name: webhook-certs\rsecret:\rsecretName: sidecar-injector-webhook-certs\r- name: webhook-config\rconfigMap:\rname: sidecar-injector-webhook-configmap apiVersion: v1\rkind: ConfigMap\rmetadata:\rname: sidecar-injector-webhook-configmap\rdata:\rsidecarconfig.yaml: |\rcontainers:\r- name: sidecar-nginx\rimage: nginx:1.12.2\rimagePullPolicy: IfNotPresent\rports:\r- containerPort: 80\rvolumeMounts:\r- name: nginx-conf\rmountPath: /etc/nginx\rvolumes:\r- name: nginx-conf\rconfigMap:\rname: nginx-configmap apiVersion: v1\rkind: Service\rmetadata:\rname: sidecar-injector-webhook-svc #2\rlabels:\rapp: sidecar-injector\rspec:\rports:\r- port: 443\rtargetPort: 443\rselector:\rapp: sidecar-injector 参考 # Diving into Kubernetes MutatingAdmissionWebhook kube-sidecar-injector git "},{"id":3,"href":"/www6vCloudNative/ref/cloudNative/k8sSpringcloud/","title":"K8s 集成Springcloud","section":"Refs","content":"\nCase1[2] # 架构 nginx 结合 springcloud gateway erueka + 本地配置 Case2[1] # 架构 k8s ingress 结合spring gateway spring-admin做服务注册, ConfigMap做服务配置 Case3[3] # 架构\nk8s ingress 结合 zuul Eureka做服务注册， ConfigServer做服务配置 网关 候选\nzuul 需要使用 Ingress中配置路由很麻烦 服务配置 候选\n环境变量 Service mysql-svc ConfigMap apollo 服务注册\n老项目 改动Eureka会有风险，建议保留 Eureka 新项目 [4] 用istio envoy side 做 服务发现 候选 Service 基于CoreDNS的服务发现 Env 基于环境变量的服务发现 参考 # 云原生Java架构师的第一课K8s+Docker+KubeSphere+DevOps 166 V 项目Repo git\nSpringCloud微服务电商系统在Kubernetes集群中上线详细教程 用的都是Deployment ，需要部署成StatefulSet\nkubernetes架构师课程 P195-P200 V\nGo to Page self\n"},{"id":4,"href":"/www6vCloudNative/ref/cloudNative/k8sAvailableHealth/","title":"K8S高可用-零停机[探针]","section":"Refs","content":"\n健康检查 # Liveness Probe [2] # 确定何时重启容器. 例如，当应用程序处于运行状态但无法做进一步操作，liveness探针将捕获到deadlock，重启处于该状态下的容器，使应用程序在存在bug的情况下依然能够继续运行下去。\nliveness的初始值为成功。\nReadiness Probe [2] # 确定容器是否已经就绪可以接受流量. 该信号的作用是控制哪些Pod应该作为service的后端。如果Pod处于非就绪状态，那么它们将会被从service的load balancer中移除。\nreadiness的初始值为失败。\nstartupProbe [4] # 启动检查, 使用启动探针检测容器应用程序是否已经启动\n对于较新的（≥v1.16）Kubernetes 集群，如果是具有不可预测或可变启动时间的应用程序应使用 startup 探针。\n只运行一次。\n探针类型\nhttpGet: 指定端口和路径执行 HTTP GET 请求\ntcpSocket: 对容器的 IP 地址上的指定端口执行 TCP 检查\n命令,exec: 在容器内执行指定命令 优雅终止 [5] # 系统底层默认会向主进程发送 SIGTERM 信号，而对剩余子进程发送 SIGKILL 信号。系统这样做的大概原因是因为大家在设计主进程脚本的时候都不会进行信号的捕获和传递，这会导致容器关闭时，多个子进程无法被正常终止，所以系统使用 SIGKILL 这个不可屏蔽信号，而是为了能够在没有任何前提条件的情况下，能够把容器中所有的进程关掉。\n也就是说如果主进程自身不是服务本身，可能会导致是被强制Kill的，解决的方法也很简单，也就是在主进程中对收到的信号做个转发，发送到容器中的其他子进程，这样容器中的所有进程在停止时，都会收到 SIGTERM，而不是 SIGKILL 信号了。\n代码 [6] # Probe apiVersion: v1 kind: Pod metadata: name: nginx namespace: default spec: containers: - name: nginx image: nginx # 存活检测 livenessProbe: failureThreshold: 3 initialDelaySeconds: 30 periodSeconds: 30 successThreshold: 1 tcpSocket: port: 5084 timeoutSeconds: 1 # 就绪检测 readinessProbe: failureThreshold: 3 initialDelaySeconds: 30 periodSeconds: 30 successThreshold: 1 tcpSocket: port: 5084 timeoutSeconds: 1 # 优雅退出 lifecycle: preStop: exec: command: - sleep - 30 terminationGracePeriodSeconds: 60 Service\nCluster 模式（externalTrafficPolicy: Cluster） apiVersion: v1 kind: Service metadata: name: nginx namespace: default spec: externalTrafficPolicy: Cluster ### ports: - port: 80 protocol: TCP targetPort: 80 selector: run: nginx type: LoadBalancer Local 模式（externalTrafficPolicy: Local）\napiVersion: v1 kind: Service metadata: name: nginx namespace: default spec: externalTrafficPolicy: Local ### ports: - port: 80 protocol: TCP targetPort: 80 selector: run: nginx type: LoadBalancer 参考 # 健康检查\nLiveness和Readiness两种Health Check手段在Kubernetes中的使用 耕耘实录 配置Pod的liveness和readiness探针 宋净超 liveness与readiness的探针工作方式源码解析 xinkun的博客 Kubernetes 探针详解！ 04 | 理解进程(3):为什么我在容器中的进程被强制杀死了? - 李程远 更新应用时，如何实现 K8s 零中断滚动更新 *** "},{"id":5,"href":"/www6vCloudNative/ref/cloudNative/k8sCalico1/","title":"Calico","section":"Refs","content":"\nBest Practice # At a high-level, the key recommendations are: # Use the Kubernetes datastore. 使用K8S datastore Install Typha to ensure datastore scalability. 安装Typha Use no encapsulation for single subnet clusters. 在单一子网的集群中，不要封装 Use IP-in-IP in CrossSubnet mode for multi-subnet clusters. 在多子网集群中，使用IP-in-IP的跨子网模式 Configure Calico MTU based on the network MTU and the chosen routing mode. MTU Add global route reflectors for clusters capable of growing above 50 nodes. 50个nodes以上使用RR(route reflectors) Use GlobalNetworkPolicy for cluster-wide ingress and egress rules. Modify the policy by adding namespace-scoped NetworkPolicy. Calico Component Overview # calico-node # Route programming: Based on known routes to pods in the Kubernetes cluster, configure the Linux host to facilitate routing accordingly. [路由配置 Flex] Route sharing: Based on pods running on this host, provide a mechanism to share known routes with other hosts. Typically accomplished with (BGP) Border Gateway Protocol. [路由共享 BIRD] calico-kube-controller # The calico-kube-controller is responsible for recognizing changes in Kubernetes objects that impact routing.\n识别影响路由变化。且包含多个控制器 multiple controllers policy-controller ns-controller sa-controller pod-controller node-controller Typha # Calico Datastore # Calico supports 2 datastore modes, Kubernetes and etcd. Kubernetes Datastore Mode (Recommended) etcd Datastore Mode Routing Configuration # Routing Methods # 3 routing modes\nNative: Packets routed as-is, no encapsulation. IP-in-IP: Minimal encapsulation; outer header includes host source/destination IPs and inner header includes pod source/destination. [tunl] VXLAN: Robust encapsulation using UDP over IP; outer header includes host source/destination IP addresses and inner header includes pod source/destination IP addresses as well as Ethernet headers. [on udp, ] 1.注意VxLAN模式不需要BGP协议参与！！！但是IPIP模式是需要的。 Calico supports two types of encapsulation: VXLAN and IP in IP. VXLAN is supported in some environments where IP in IP is not (for example, Azure). VXLAN has a slightly higher per-packet overhead because the header is larger, but unless you are running very network intensive workloads the difference is not something you would typically notice. The other small difference between the two types of encapsulation is that Calico’s VXLAN implementation does not use BGP, whereas Calico’s IP in IP implementation uses BGP between Calico nodes. # From Calico Docs\nSingle Subnet Configuration # Multi-Subnet Configuration # Route Distribution # BGP Full Mesh node-to-node mesh Global Route Reflection(RR) Node-Specific Route Reflection Peering can be configured for BGP-capable hardware in a datacenter’s network. Most commonly setup with top of rack (ToR) switches. 参考 # Calico Reference Architecture 20210808-Calico IPIP Mode "},{"id":6,"href":"/www6vCloudNative/ref/cloudNative/k8sVM/","title":"Kubernetes和VM","section":"Refs","content":"\n参考 # kubevirt在360的探索之路（k8s接管虚拟化）\n后Kubernetes时代的虚拟机管理技术之kubevirt篇\n"},{"id":7,"href":"/www6vCloudNative/ref/cloudNative/k8sAdvancedScheduling/","title":"Kubernetes 高级调度","section":"Refs","content":"\n高级调度 Overview [2] # Affinity vs. Taint [2] # 就污点而言,它的使用通常是负向的, 也就说, 污点常用在某Node不让大多数Pod调度只让少部分Pod调度时,又或者节点根本不参加工作负载时。比如:我们常见的master节点上不调度负载pod,保证master组件的稳定性；节点有特殊资源，大部分应用不需要而少部分应用需要,如GPU。\n就Node Affinity来说,他的使用可以正向的,也就是说,我们想让某个应用的Pod部署在指定的一堆节点上。当然,也可以是负向的,比如说我们常说的Node 反亲和性,只需要把操作符设置为NotIn就能达成预期目标。\n就污点而言，如果节点设置的污点效果是NoSchedule或者NoExecute,意味着没有设置污点容忍的Pod绝不可能调度到这些节点上。\n就Node Affinity而言,如果节点设置了Label,但是Pod没有任何的Node Affinity设置,那么Pod是可以调度到这些节点上的。\n特性 默认 优/劣势 taint 负向的 设置NoSchedule， 默认不可调度 不要改现有pod[2] 亲和，反亲和 正向的 设置了Label， 默认可调度 要改现有pod[2] 亲和性 # NodeAffinity配置[1]\nNodeAffinity配置分类: requiredDuringSchedulingIgnoredDuringExecution (强亲和性) preferredDuringSchedulingIgnoredDuringExecution (首选亲和性) Topology [3] # topologyKey\n参考 # Kubernetes高级调度- Taint和Toleration、Node Affinity分析 详解 K8S Pod 高级调度 kubernetes架构师课程 P97 P98 *** 【2023】云原生Kubernetes全栈架构师：基于世界500强的k8s实战课程 1xx. Kubernetes之Pod调度 未\n"},{"id":8,"href":"/www6vCloudNative/ref/cloudNative/k8sSecurity/","title":"Kubernetes安全-Security","section":"Refs","content":"\nK8S安全加固建议 [2] # Kubernetes Pod 安全\n使用构建的容器，以非 root 用户身份运行应用程序 在可能的情况下，用不可变的文件系统运行容器 扫描容器镜像，以发现可能存在的漏洞或错误配置 使用 Pod 安全政策来执行最低水平的安全，包括: 防止有特权的容器\n拒绝经常被利用来突破的容器功能，如 hostPID、hostIPC、hostNetwork、allowedHostPath 等\n拒绝以 root 用户身份执行或允许提升为根用户的容器\n使用安全服务，如 SELinux®、AppArmor® 和 seccomp，加固应用程序，防止被利用。 @ 网络隔离和加固\n使用防火墙和基于角色的访问控制（RBAC）锁定对控制平面节点的访问 进一步限制对 Kubernetes etcd 服务器的访问 配置控制平面组件，使用传输层安全（TLS）证书进行认证、加密通信 设置网络策略来隔离资源。不同命名空间的 Pod 和服务仍然可以相互通信，除非执行额外的隔离，如网络策略 @ 将所有凭证和敏感信息放在 Kubernetes Secret 中，而不是配置文件中。使用强大的加密方法对 Secret 进行加密 认证和授权\n禁用匿名登录（默认启用） 使用强大的用户认证 创建 RBAC 策略以限制管理员、用户和服务账户活动 @ 日志审计\n启用审计记录（默认为禁用） 在节点、Pod 或容器级故障的情况下，持续保存日志以确保可用性 配置一个 metric logger 升级和应用安全实践\n立即应用安全补丁和更新 定期进行漏洞扫描和渗透测试 当组件不再需要时，将其从环境中移除 K8S安全加固最佳实践 [1] # Kubernetes 安全机制 [6] # K8S API 安全 @限制访问Kubernetes API # 所有API交互使用TLS API 认证 Kubernetes支持的请求认证方式 Basic 认证（不建议） X509 证书认证 Bearer Tokens(JSON Web Tokens)\nService Account / OpenID Connect / Webhooks API 鉴权 - RBAC @使用基于角色的访问控制来最小化暴露 三要素， 权限粒度 Role， RoleBinding ClusterRole， ClusterRoleBinding Default ClusterRoleBinding(预置角色) 容器能力限制 # 限制容器特权 Security Context\n限制容器运行时的用户、用户组，对容器特权进行限制 PSP(Pod Security Policy)\n会在 1.25 之后被后面提到的 pod security admission webhook 替代 限制资源用量 Resource Quota Limit Range 限制资源访问 network policy @使用网络安全策略来限制集群级别的访问\n网络隔离策略，设置黑名单或者白名单，为 namespace 去分配一独立的 IP 池 限制调度节点 node selector Taint 限制容器能够调度的节点，实现一定程度的物理隔离 安全增强 # 审计日志 pod security admission webhook GateKeeper 开源 Key Management Service 借助 KMS 来加密 etcd 中的数据，在容器运行时进行解密 安全容器 [1] # kata container(轻量级虚拟机) gVisor(大部分是userspace的调用) 参考：\nCNCF × Alibaba 云原生技术公开课 第27 章 ： Kubernetes安全之访问控制\n第29 章 ： 安全容器技术 Kubernetes 加固指南 *** Kubernetes in Action - 12章， 13章 （未） 记一次Kubernetes中严重的安全问题 未 Kubernetes 最佳安全实践指南 未 K8s 安全策略最佳实践 文字稿 火线沙龙第24期——K8s 安全策略最佳实践 视频 云原生安全产品 NeuVector 简介 未 "},{"id":9,"href":"/www6vCloudNative/ref/cloudNative/k8sMultiCluster/","title":"Kubernetes 多集群管理","section":"Refs","content":"\n目标 # 让用户像使用单集群一样来使用多集群。\n多集群部署需要解决哪些问题 # 而多集群管理需要解决以下问题：\n多集群服务的分发部署（deployment、daemonset等）\n跨集群自动迁移与调度（当某个集群异常，服务可以在其他集群自动部署）\n多集群服务发现，网络通信及负载均衡（service，ingress等） 开源和解决方案 # KubeFed 或 Federation v2\nkubefed 集群联邦（Cluster Federation） jimmysong virtual-kubelet Virtual Kubelet\n阿里云virtual-kubelet-autoscaler实现ECI作为弹性补充 通过 virtual-kubelet-autoscaler 将Pod自动调度到虚拟节点 自建Kubernetes集群部署Virtual Kubelet（ECI） 未 简介：Virtual Kubelet video 2018 kubecon 未 深入了解：Virtual Kubelet video 2018 kubecon 未 Karmada（Kubernetes Armada）[5]\nclusternet - 腾讯云\nOCM(Open Cluster Management) - RedHat\nGardener [3]\nCrossplane [3]\ncluster-api [4]\n参考： # k8s多集群的思考 混合云下的 Kubernetes 多集群管理与应用部署 未 【PingCAP Infra Meetup】No.141 Kubernetes 开发设计模式在 TiDB Cloud 中的应用 基于 ClusterAPI 的集群管理 Karmada: 开源的云原生多云容器编排引擎 华为 "},{"id":10,"href":"/www6vCloudNative/ref/cloudNative/k8sCalico/","title":"Calico","section":"Refs","content":"\n一. 介绍和原理 # calico 是容器网络的又一种解决方案，和其他虚拟网络最大的不同是，它没有采用 overlay 网络做报文的转发，提供了纯 3 层的网络模型。三层通信模型表示每个容器都通过 IP 直接通信，中间通过路由转发找到对方。在这个过程中，容器所在的节点类似于传统的路由器，提供了路由查找的功能。\n要想路由工作能够正常，每个虚拟路由器（容器所在的主机节点）必须有某种方法知道整个集群的路由信息，calico 采用的是 BGP 路由协议，全称是 Border Gateway Protocol。\nBGP(Border Gateway Protocol 边界网关协议): 就是在大规模网络中实现节点路由信息共享的一种协议\nBGP 协议传输的消息\r+ 1 [BGP 消息]\r+ 2 我是宿主机 192.168.1.3\r+ 3 10.233.2.0/24 网段的容器都在我这里 + 4 这些容器的下一跳地址是我 二. 组件 # Calico 的 CNI 插件\nFelix 它是一个 DaemonSet，负责在宿主机上插入路由规则(即:写入 Linux 内核的 FIB 转发信息库)，以及维护 Calico 所需的网络设备等工作。\n路由规则(核心) \u0026lt;目的容器 IP 地址段\u0026gt; via \u0026lt;网关的 IP 地址\u0026gt; dev eth0 iptables的配置组件Felix; 基于iptable/linux kernel包转发; 根据iptables规则进行路由转发; BIRD， 路由广播组件BGP Speaker BIRD是 BGP 的客户端，专门负责在集群里分发路由规则信息。 三. 架构 # 1. Node-to-Node Mesh模式（小规模） # 默认配置下，是一个被称为“Node-to-Node Mesh”的模式，一般推荐用在少于 100 个节点的集群里 Node 称为 BGP Peer 非overlay, Calico 没有使用 CNI 的网桥模式; 宿主机 Node 2 上的 Container 4 对应的路由规则，如下所示: 10.233.2.3 dev cali5863f3 scope link 即:发往 10.233.2.3 的 IP 包，应该进入 cali5863f3 设备。 2. Route Reflector模式 + IPIP模式（大规模） # 默认情况下，每个 calico 节点会和集群中其他所有节点建立 BGP peer 连接，也就是说这是一个 O(n^2) 的增长趋势。在集群规模比较小的情况下，这种模式是可以接受的，但是当集群规模扩展到百个节点、甚至更多的时候，这样的连接数无疑会带来很大的负担。为了解决集群规模较大情况下 BGP client 连接数膨胀的问题，calico 引入了 RR（Router Reflector） 的功能。\nRR 的基本思想是选择一部分节点（一个或者多个）作为 Global BGP Peer，它们和所有的其他节点互联来交换路由信息，其他的节点只需要和 Global BGP Peer 相连就行，不需要之间再两两连接。更多的组网模式也是支持的，不管怎么组网，最核心的思想就是所有的节点能获取到整个集群的路由信息。 参考 # 《31容器网络之Calico：为高效说出善意的谎言》 趣谈网络协议 刘超\n《35 解读Kubernetes三层网络方案》 深入剖析Kubernetes 张磊\nkubernetes网络之\u0026mdash;Calico原理解读 看图\n容器网络Calico进阶实践 | 褚向阳 \u0026ldquo;看看 Calico 是如何实现跨主机互通\u0026rdquo;\nCalico网络方案 安装\ndocker 容器网络方案：calico 网络模型 安装+原理 - 阿里人 - ”报文流程“\n\u0026laquo;kubernetes网络权威指南\u0026raquo; 5.4节\n20210806-Calico基础架构 未\nConfigure BGP peering 未 Full-mesh , Route reflectors\nOverlay networking 未 公有云环境中（aws） ipipMode field (IP in IP encapsulation)， ipipMode 必须with BGP vxlanMode field (VXLAN encapsulation)， vxlanMode 可以without BGP 两种模式不能一起运行，只能运行其中的一种\nCalico 路由反射模式权威指南 未\n"},{"id":11,"href":"/www6vCloudNative/ref/cloudNative/observabilityPrometheus/","title":"可观测性-Prometheus","section":"Refs","content":"\nMetric 之 Prometheus [9] # 非分布式， 联邦 pushgataway 服务发现 拉模式 业务场景 # 业务指标 metric 多云, 多地域监控 # 在远端云上部署Prometheus agent\nPrometheus 存储层的演进 [7] # 1st Generation: Prototype key metric name labels timestamp value 2nd Generation: Prometheus V1 压缩 Timestamp Compression: Double Delta Value Compression Chunk Encoding\n1KB 3rd Generation: Prometheus V2 Prometheus服务发现机制[10][11] # kubernetes_sd_configs: #基于 Kubernetes API实现的服务发现，让prometheus动态发现kubernetes中被监控的目标 static_configs: #静态服务发现，基于prometheus配置文件指定的监控目标 dns_sd_configs: #DNS服务发现监控目标 consul_sd_configs: #Consul服务发现，基于consul服务动态发现监控目标 file_sd_configs: #基于指定的文件实现服务发现，基于指定的文件发现监控目标 参考： # 第十八期: 玩转云原生容器场景的Prometheus监控 腾讯云 云原生正发声 #todo 重看一遍 存储层 # Prometheus 存储层的演进 *** Metric # 《48 | Prometheus、Metrics Server与Kubernetes监控体系》 深入剖析Kubernetes 张磊 《第七模块 ：微服务监控告警Prometheus架构和实践 119.监控模式分类》 微服务架构实战160讲 杨波 partial 服务发现机制 # kubernetes架构师课程 159 V *** prometheus服务发现 "},{"id":12,"href":"/www6vCloudNative/ref/cloudNative/etcd/","title":"etcd 总结","section":"Refs","content":"\n读流程 [3] # 一个读请求从 client 通过 Round-robin 负载均衡算法，选择一个-etcd server 节点，发出 gRPC 请求，经过 etcd server 的 KVServer 模块、线性读模块、MVCC 的 treeIndex 和 boltdb 模块紧密协作，完成了一个读请求。\netcd 提供的两种读机制 (串行读和线性读)\n写流程[4] # etcd 的写请求流程，重点介绍了 Quota、WAL、Apply 和 MVCC 模块。Quota 模块会防止 db 大小超限，WAL 模块保证了集群的一致性和可恢复性，Apply 模块实现了幂等性，MVCC 模块维护索引版本号和内存索引结构。etcd 通过异步、批量提交事务机制提升写 QPS 和吞吐量。这些模块相互协作，实现了在节点遭遇 crash 等异常情况下，不丢任何已提交的数据、不重复执行任何提案。[N AI]\n客户端Lease [5] # etcd 的 Lessor 模块在启动时会创建两个常驻 goroutine，分别用于检查过期 Lease 并撤销，以及定时更新 Lease 的剩余到期时间。该模块提供了 Grant、Revoke、LeaseTimeToLive 和 LeaseKeepAlive API，用于创建、撤销、获取有效期和续期 Lease。[N AI]\nMVCC 和 revision[2] # etcd revision 定义 计数器 键值空间发生变化， revision相应增加 用途 逻辑时钟 MVCC 参考 # 第16 章 ： 深入理解 etcd - 基本原理解析 未 《云原生分布式存储基石-etcd深入解析》 《02 | 基础架构：etcd一个读请求是如何执行的？》 《03丨基础架构：etcd一个写请求是如何执行的？》 《06 | 租约：如何检测你的客户端存活？》 "},{"id":13,"href":"/www6vCloudNative/ref/cloudNative/k8sAvailable/","title":"K8S高可用-零停机[自主中断]","section":"Refs","content":"\n自主中断和非自主中断[4] # PodDisruptionBudged [4] # 无状态应用： maxUnavailable = 40% 单实例有状态应用: 多实例有状态应用： etcd N, maxUnavailable=1 或者 minAvailable=N 参考 # 模块十一： 将应用迁移至Kubernetes平台 使用 PDB 避免 Kubernetes 集群中断 未 "},{"id":14,"href":"/www6vCloudNative/ref/cloudNative/k8sOperator-redis/","title":"Kubernetes Operator-Redis","section":"Refs","content":"\nDeploy redis cluster operator # kubectl create -f deploy/crds/redis.kun_distributedredisclusters_crd.yaml\rkubectl create -f deploy/crds/redis.kun_redisclusterbackups_crd.yaml\r// cluster-scoped\r$ kubectl create -f deploy/service_account.yaml\r$ kubectl create -f deploy/cluster/cluster_role.yaml\r$ kubectl create -f deploy/cluster/cluster_role_binding.yaml\r$ kubectl create -f deploy/cluster/operator.yaml Deploy a sample Redis Cluster # ## Custom Resource\r$ kubectl create -f deploy/example/custom-resources.yaml\r## Persistent Volume\r$ kubectl create -f deploy/example/persistent.yaml 参考 # redis-cluster-operator kubernetes架构师课程 129 V ***\n"},{"id":15,"href":"/www6vCloudNative/ref/cloudNative/observabilityPrometheusHA/","title":"可观测性-Prometheus  HA","section":"Refs","content":"\n高可用方案[1] # 高可用的几种方案 # 基本 HA：即两套 Prometheus 采集完全一样的数据，外边挂负载均衡 HA + 远程存储：除了基础的多副本 Prometheus，还通过 Remote Write 写入到远程存储，解决存储持久化问题 联邦集群：即 Federation，按照功能进行分区，不同的 Shard 采集不同的数据，由Global节点来统一存放，解决监控数据规模的问题。 使用 Thanos 或者 Victoriametrics，来解决全局查询、多副本数据 Join 问题。 问题 # 就算使用官方建议的多副本 + 联邦，仍然会遇到一些问题:\n官方建议数据做 Shard，然后通过Federation来实现高可用， 但是边缘节点和Global节点依然是单点，需要自行决定是否每一层都要使用双节点重复采集进行保活。 也就是仍然会有单机瓶颈。 另外部分敏感报警尽量不要通过Global节点触发，毕竟从Shard节点到Global节点传输链路的稳定性会影响数据到达的效率，进而导致报警实效降低。 例如服务Updown状态，Api请求异常这类报警我们都放在Shard节点进行报警。 原因 # 本质原因是，Prometheus 的本地存储没有数据同步能力，要在保证可用性的前提下，再保持数据一致性是比较困难的，基础的 HA Proxy 满足不了要求，比如：\n集群的后端有 A 和 B 两个实例，A 和 B 之间没有数据同步。A 宕机一段时间，丢失了一部分数据，如果负载均衡正常轮询，请求打到A 上时，数据就会异常。 如果 A 和 B 的启动时间不同，时钟不同，那么采集同样的数据时间戳也不同，就不是多副本同样数据的概念了 就算用了远程存储，A 和 B 不能推送到同一个 TSDB，如果每人推送自己的 TSDB，数据查询走哪边就是问题了。 解决方案 # 在存储、查询两个角度上保证数据的一致 存储角度：如果使用 Remote Write 远程存储， A 和 B后面可以都加一个 Adapter，Adapter做选主逻辑，只有一份数据能推送到 TSDB，这样可以保证一个异常，另一个也能推送成功，数据不丢，同时远程存储只有一份，是共享数据。方案可以参考这篇文章 查询角度：上边的方案实现很复杂且有一定风险，因此现在的大多数方案在查询层面做文章，比如 Thanos 或者 Victoriametrics，仍然是两份数据，但是查询时做数据去重和 Join。只是 Thanos 是通过 Sidecar 把数据放在对象存储，Victoriametrics 是把数据 Remote Write 到自己的 Server 实例，但查询层 Thanos-Query 和 Victor 的 Promxy的逻辑基本一致。 Thanos # 组件 # Bucket Check Compactor Query Rule Sidecar Store receive downsample 特点 # 为多集群Prometheus提供全局接口\n全局视图 将监控数据存储到各种对象存储 为Prometheus提供高可用 易于与Prometheus集成并可模块化部署 模式 # Sidecar(默认的模式) pull, remote read Receive\npush, remote rewrite 参考 # HA # 高可用 Prometheus：问题集锦 高可用 Prometheus：Thanos 实践 第十八期: 玩转云原生容器场景的Prometheus监控 腾讯云 云原生正发声 #todo 重看一遍 Thanos：开源的大规模Prometheus集群解决方案 失效 基于 KubeSphere 和 Thanos 构建可持久化存储的多集群监控系统 失效 打造云原生大型分布式监控系统(二): Thanos 架构详解 imroc@腾讯云 *** Thanos与Cortex方案对比 未 Prometheus Thanos与Cortex组件比较 未 "},{"id":16,"href":"/www6vCloudNative/ref/cloudNative/k8sIngressNginx/","title":"Kubernetes Nginx Ingress","section":"Refs","content":"\nKubernetes Nginx Ingress [0] # 版本1-K8s官方维护 [1][2] # There are three ways to customize NGINX: ConfigMap: using a Configmap to set global configurations in NGINX. [滚动更新后生效， 针对全局的配置] Annotations: use this if you want a specific configuration for a particular Ingress rule. [立即生效，针对某个域名location进行配置] Custom template: when more specific settings are required, like open_file_cache, adjust listen options as rcvbuf or when is not possible to change the configuration through the ConfigMap. 版本2-Nginx官方维护 [3][4] # 版本1 vs. 版本2 [5] # 实践 [0] # 域名重定向 *** # redirect ， http 重定向到https， 新旧域名替换\n前后端分离 *** # rewrite\n+ www.a.com / --\u0026gt; 前端服务\r+ www.a.com /api --\u0026gt; 后端服务 /api www.a.com/api rewrite到 www.a.com SSL配置 [p] # 验证 自签名证书\n匹配请求头 # Mobile 和 PC 的请求头不同，路由到不同的后端服务\n实现灰度金丝雀发布[6][7] # 不同灰度方式的优先级由高到低为： canary-by-header\u0026gt;canary-by-cookie\u0026gt;canary-weight\nAnnotation 说明 nginx.ingress.kubernetes.io/canary 必须设置该Annotation值为true，否则其它规则将不会生效。 nginx.ingress.kubernetes.io/canary-by-header 表示基于请求头的名称进行灰度发布。 nginx.ingress.kubernetes.io/canary-by-header-value 表示基于请求头的值进行灰度发布。 nginx.ingress.kubernetes.io/canary-by-header-pattern 表示基于请求头的值进行灰度发布，并对请求头的值进行正则匹配。 nginx.ingress.kubernetes.io/canary-by-cookie 表示基于Cookie进行灰度发布。 nginx.ingress.kubernetes.io/canary-weight 表示基于权重进行灰度发布。 nginx.ingress.kubernetes.io/canary-weight-total 表示设定的权重总值。 %accordion%灰度发布示例%accordion%\n基于Header灰度（自定义header值）：当请求Header为ack: alibaba时将访问灰度服务；其它Header将根据灰度权重将流量分配给灰度服务。 基于Cookie灰度：当Header不匹配时，请求的Cookie为hangzhou_region=always时将访问灰度服务。 apiVersion: networking.k8s.io/v1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/canary: \u0026ldquo;true\u0026rdquo; nginx.ingress.kubernetes.io/canary-weight: \u0026ldquo;20\u0026rdquo; nginx.ingress.kubernetes.io/canary-by-header: \u0026ldquo;ack\u0026rdquo; nginx.ingress.kubernetes.io/canary-by-header-value: \u0026ldquo;alibaba\u0026rdquo; nginx.ingress.kubernetes.io/canary-by-cookie: \u0026ldquo;hangzhou_region\u0026rdquo; %/accordion%\n速率限制 # 黑白名单 # allow deny\n自定义错误页面 # 参考 # kubernetes架构师课程 162 V *** Ingress-Nginx Doc Ingress-Nginx Github Nginx-ingress Doc Nginx-ingress GitHub nginx-ingress-controllers.md 表格 Nginx Ingress高级用法 【IT老齐294】大厂如何基于K8S实现金丝雀发布 "},{"id":17,"href":"/www6vCloudNative/ref/cloudNative/k8sAppMigrate/","title":"K8S 应用迁移至K8S","section":"Refs","content":"\n应用容器化 # 启动速度 # 健康检查 # 启动参数 # Dockerfile # 基础镜像，Utility Lib 主进程, Fork bomb 代码和配置分离\n配置： 环境变量， 配置文件mount 分层控制 Entrypoint 日志 # Log Driver\nBlocking mode\nNon Blocking mode sideCar模式 Node 模式\n容器日志采集利器Log-Pilot 共享kernel # 系统参数配置共享\n虚拟机和容器中的内核参数 kernel\nK8S 内核参数 进程数共享 - Fork bomb\nhttps://github.com/krallin/tini fd数共享 容器可能用完主机所有的fd 主机磁盘共享\nlvm， 强IO 容器化应用的资源监控 # 容器中看到的是主机资源\ntop\njava runtime.GetAvailableProcess() - cpu数\ncat /proc/cupinfo\ncat /proc/meminfo\ndf -k 解决方案 查询/proc/1/cgroup是否包含kubepods关键字，表明运行在k8s之上。（参考1） 要得到单个容器的 CPU 使用率，我们可以从 CPU Cgroup 每个控制组里的统计文件 cpuacct.stat 中获取。单个容器 CPU 使用率 =((utime_2 – utime_1) + (stime_2 – stime_1)) * 100.0 / (HZ * et * 1 )。（参考2） 其他 lxcfs 对应用的影响\nJava - Concurrent GC Thread， Heap Size， 线程数不可控 Pod Spec # 初始化需求(init container)\n需要几个主container\n权限？ Privilege和Security(PSP)\n共享哪些namespace（PID，IPC，NET，UTS，MNT）\nNET 默认共享\n配置管理\n环境变量\nvolumn mount\nDNS策略及对resolv.conf的影响\ndefault\nclsuterFirst: 默认 coredns\nclusterFirstWithHostNet\nnone\nimagePullPolicy Image 拉取策略\nNever，IfNotPresent， always\n数据保存 local-ssd: 独占的本地磁盘， 独占io， 固定大小， 读写性能高;\nlocal-dynamic: LVM,动态分配空间， 效率低;\n可用性 self\n参考： # 模块十一： 将应用迁移至Kubernetes平台 06|容器CPU(2):如何正确地拿到容器CPU的开销? - 李程远 DEVOPS-迁移SpringMVC应用到生产K8S集群 未 java项目 如何把应用程序迁移到k8s 未 golang项目 "},{"id":18,"href":"/www6vCloudNative/ref/cloudNative/k8sObservability/","title":"可观测性-Kubernetes","section":"Refs","content":"\nKubernetes可观察性 [1] # NPD： node problem detector kube-eventer： Kubernetes 事件离线工具 Kubernetes Metrics监控方案 [2] # cadvisor/exporter+prometheus+grafana\nKubernetes Logs监控方案[3][4] # %accordion%es-statefulset.yaml%accordion%\napiVersion: apps/v1 kind: StatefulSet metadata: name: elasticsearch-logging namespace: logging labels: k8s-app: elasticsearch-logging version: v7.10.2 addonmanager.kubernetes.io/mode: Reconcile spec: serviceName: elasticsearch-logging replicas: 2 selector: matchLabels: k8s-app: elasticsearch-logging version: v7.10.2 template: metadata: labels: k8s-app: elasticsearch-logging version: v7.10.2 spec: serviceAccountName: elasticsearch-logging containers: - image: quay.io/fluentd_elasticsearch/elasticsearch:v7.10.2 name: elasticsearch-logging imagePullPolicy: Always ... ports: - containerPort: 9200 name: db protocol: TCP - containerPort: 9300 name: transport protocol: TCP volumeMounts: - name: elasticsearch-logging mountPath: /data env: - name: \u0026#34;NAMESPACE\u0026#34; valueFrom: fieldRef: fieldPath: metadata.namespace - name: \u0026#34;MINIMUM_MASTER_NODES\u0026#34; value: \u0026#34;1\u0026#34; volumes: - name: elasticsearch-logging emptyDir: {} ### 生产上不要挂在本地 # Elasticsearch requires vm.max_map_count to be at least 262144. # If your OS already sets up this number to a higher value, feel free # to remove this init container. initContainers: - image: alpine:3.6 command: [\u0026#34;/sbin/sysctl\u0026#34;, \u0026#34;-w\u0026#34;, \u0026#34;vm.max_map_count=262144\u0026#34;] name: elasticsearch-logging-init securityContext: privileged: true %/accordion%\n%accordion%fluentd-es-configmap.yaml%accordion%\n\u0026lt;source\u0026gt;\r@id fluentd-containers.log\r@type tail\rpath /var/log/containers/*.log ### 宿主机映射到容器的目录\rpos_file /var/log/es-containers.log.pos ### position日志,消费偏移量\rtag raw.kubernetes.*\rread_from_head true\r\u0026lt;parse\u0026gt;\r@type multi_format\r\u0026lt;pattern\u0026gt;\rformat json\rtime_key time\rtime_format %Y-%m-%dT%H:%M:%S.%NZ\r\u0026lt;/pattern\u0026gt;\r\u0026lt;pattern\u0026gt;\rformat /^(?\u0026lt;time\u0026gt;.+) (?\u0026lt;stream\u0026gt;stdout|stderr) [^ ]* (?\u0026lt;log\u0026gt;.*)$/\rtime_format %Y-%m-%dT%H:%M:%S.%N%:z\r\u0026lt;/pattern\u0026gt;\r\u0026lt;/parse\u0026gt;\r\u0026lt;/source\u0026gt; output.conf: |-\r\u0026lt;match **\u0026gt;\r@id elasticsearch\r@type elasticsearch\r@log_level info\rtype_name _doc\rinclude_tag_key true\rhost elasticsearch-logging ### service的名字[集群内部的]\rport 9200\rlogstash_format true\r\u0026lt;buffer\u0026gt;\r@type file\rpath /var/log/fluentd-buffers/kubernetes.system.buffer\rflush_mode interval\rretry_type exponential_backoff\rflush_thread_count 2\rflush_interval 5s\rretry_forever\rretry_max_interval 30\rchunk_limit_size 2M\rtotal_limit_size 500M\roverflow_action block\r\u0026lt;/buffer\u0026gt;\r\u0026lt;/match\u0026gt; %/accordion%\n%accordion%fluentd-es-ds.yaml%accordion%\napiVersion: apps/v1\rkind: DaemonSet\rmetadata:\rname: fluentd-es-v3.1.1\rnamespace: logging\rlabels:\rk8s-app: fluentd-es\rversion: v3.1.1\raddonmanager.kubernetes.io/mode: Reconcile\rspec:\rselector:\rmatchLabels:\rk8s-app: fluentd-es\rversion: v3.1.1\rtemplate:\rmetadata:\rlabels:\rk8s-app: fluentd-es\rversion: v3.1.1\rspec:\rsecurityContext:\rseccompProfile:\rtype: RuntimeDefault\rpriorityClassName: system-node-critical\rserviceAccountName: fluentd-es\rcontainers:\r- name: fluentd-es\rimage: quay.io/fluentd_elasticsearch/fluentd:v3.1.0\renv:\r- name: FLUENTD_ARGS\rvalue: --no-supervisor -q\r...\rvolumeMounts:\r- name: varlog\rmountPath: /var/log\r- name: varlibdockercontainers\rmountPath: /var/lib/docker/containers ### 收集打到控制台的日志\rreadOnly: true\r- name: config-volume\rmountPath: /etc/fluent/config.d\rports:\r- containerPort: 24231\rname: prometheus\rprotocol: TCP\r...\rterminationGracePeriodSeconds: 30\rvolumes:\r- name: varlog\rhostPath:\rpath: /var/log\r- name: varlibdockercontainers\rhostPath:\rpath: /var/lib/docker/containers\r- name: config-volume\rconfigMap:\rname: fluentd-es-config-v0.2.1 %/accordion%\n参考 # 第12 章 ： 可观测性：监控与日志 Prometheus监控k8s Prometheus监控方案 fluentd-elasticsearch EFK监控方案 kubernetes架构师课程 V 143 *** "},{"id":19,"href":"/www6vCloudNative/ref/cloudNative/k8sUpgrade/","title":"Kubernetes 升级upgrade","section":"Refs","content":"\n宏观升级流程 # 1、升级主master节点\n2、升级其他master节点\n3、升级node节点\n微观升级步骤 # 1、先升级kubeadm版本\n2、升级第一个主控制平面节点Master组件\n3、升级第一个主控制平面节点上的Kubelet和kubectl\n4、升级其他控制平面节点\n5、升级Node节点\n6、验证集群\n升级注意事项 # 确定升级前的的kubeadm集群版本。 kubeadm upgrade不会影响到工作负载，仅涉及k8s内部的组件，但是备份etcd数据库是最佳实践。 升级后，所有容器都会重启动，因为容器的hash值已更改。 由于版本的兼容性，只能从一个次要版本升级到另外一个次要版本，不能跳跃升级。 集群控制平面应使用静态Pod和etcd pod或外部etcd。 参考： # Kubernetes版本升级实践 Kubernetes升级：自己动手的权威指南\n"},{"id":20,"href":"/www6vCloudNative/ref/cloudNative/k8sSecurityPractice/","title":"Kubernetes 安全实践","section":"Refs","content":"\nMicrosoft的Kubernetes attack matrix [1] # Kubernetes attack matrix-enhancement [2] # 初始访问 # ● API Server 未授权访问 [5]\nAPI Server 作为 K8s 集群的管理入口，通常使用 8080 和 6443 端口，其中 8080 端口无需认证，6443端口需要认证且有 TLS 保护。如果开发者使用 8080 端口，并将其暴露在公网上，攻击者就可以通过该端口的 API，直接对集群下发指令。\n另一种场景是运维人员配置不当，将\u0026quot;system:anonymous\u0026quot;用户绑定到\u0026quot;cluster-admin\u0026quot;用户组，从而使6443端口允许匿名用户以管理员权限向集群内部下发指令。\n● kubelet 未授权访问 [5]\n● Docker Daemon 公网暴露\n● K8s configfile 泄露\nK8s configfile 作为 K8s 集群的管理凭证，其中包含有关 K8s 集群的详细信息（API Server、登录凭证）。 如果攻击者能够访问到此文件(如办公网员工机器入侵、泄露到 Github的代码等)，就可以直接通过 API Server 接管 K8s 集群，带来风险隐患。 拿到K8s configfile完整利用流程\nK8s configfile \u0026ndash;\u0026gt; 创建后门Pod/挂载主机路径 \u0026ndash;\u0026gt; 通过Kubectl 进入容器 \u0026ndash;\u0026gt; 利用挂载目录逃逸。 执行 # ● 利用Service Account 容器内部默认携带 K8s Service Account的认证凭据,路径为：/run/secrets/kubernetes.io/serviceaccount/token\n如运维配置不当没有设置 RBAC （基于角色的访问控制）,那么攻击者就可以通过 Pod 获取到 Token 进行API Server认证。\n\u0026hellip;创建特权Pod\u0026hellip;\n● CURL方式请求\n● kubectl方式请求\n持久化 # ● DaemonSets、Deployments\n● Shadow API\n● Rootkit\n● cronjob持久化\n权限提升 # ● 特权容器逃逸 [3]\n「容器逃逸」指这样的一种过程和结果：首先，攻击者通过劫持容器化业务逻辑，或直接控制（CaaS等合法获得容器控制权的场景）等方式，已经获得了容器内某种权限下的命令执行能力；攻击者利用这种命令执行能力，借助一些手段进一步获得该容器所在直接宿主机（经常见到“物理机运行虚拟机，虚拟机再运行容器”的场景，该场景下的直接宿主机指容器外层的虚拟机）上某种权限下的命令执行能力。\n当操作者执行docker run \u0026ndash;privileged时，Docker将允许容器访问宿主机上的所有设备，同时修改AppArmor或SELinux的配置，使容器拥有与那些直接运行在宿主机上的进程几乎相同的访问权限。\n● Docker漏洞\n● Linux Capabilities逃逸\n探测 # ● 内网扫描\n● K8s常用端口探测\n● 集群内部网络\n横向移动 # ● 污点(Taint)横向渗透\n参考 # Threat matrix for Kubernetes overview 云原生之 Kubernetes 安全 深信服千里目安全实验室 overview 容器逃逸技术概览 *** 红队视角下的容器逃逸利用及分析 未 浅析K8S各种未授权攻击方法 k8s对外攻击面总结 未 "},{"id":21,"href":"/www6vCloudNative/ref/cloudNative/k8sCKS/","title":"Kubernetes CKS","section":"Refs","content":"\nCluster Setup - 10% # Securing a Cluster # 使用网络安全策略来限制集群级别的访问 Use Network security policies to restrict cluster level access 使用CIS基准检查Kubernetes组件(etcd, kubelet, kubedns, kubeapi)的安全配置 Use CIS benchmark to review the security configuration of Kubernetes components (etcd, kubelet, kubedns, kubeapi) Kube-bench - Checks whether Kubernetes is deployed securely by running the checks documented ain the CIS Kubernetes Benchmark. 正确设置带有安全控制的Ingress对象 Properly set up Ingress objects with security control 保护节点元数据和端点 Protect node metadata and endpoints Using Kubernetes network policy to restrict pods access to cloud metadata This example assumes AWS cloud, and metadata IP address is 169.254.169.254 should be blocked while all other external addresses are not. apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: deny-only-cloud-metadata-access spec: podSelector: {} policyTypes: - Egress egress: - to: - ipBlock: cidr: 0.0.0.0/0 except: - 169.254.169.254/32 最小化GUI元素的使用和访问 Minimize use of, and access to, GUI elements\n在部署之前验证平台二进制文件 Verify platform binaries before deploying\nKubernetes binaries can be verified by their digest **sha512 hash** Checking the Kubernetes release page for the specific release Checking the change log for the images and their digests Cluster Hardening - 15% # 限制访问Kubernetes API Restrict access to Kubernetes API Control anonymous requests to Kube-apiserver Non secure access to the kube-apiserver 使用基于角色的访问控制来最小化暴露 Use Role-Based Access Controls to minimize exposure Handy site collects together articles, tools and the official documentation all in one place Simplify Kubernetes Resource Access Control using RBAC Impersonation 谨慎使用服务帐户，例如禁用默认设置，减少新创建帐户的权限 Exercise caution in using service accounts e.g. disable defaults, minimize permissions on newly created ones Opt out of automounting API credentials for a service account Opt out at service account scope # apiVersion: v1 kind: ServiceAccount metadata: name: build-robot automountServiceAccountToken: false Opt out at pod scope # apiVersion: v1 kind: Pod metadata: name: cks-pod spec: serviceAccountName: default automountServiceAccountToken: false 经常更新Kubernetes Update Kubernetes frequently System Hardening - 15% # 最小化主机操作系统的大小(减少攻击面) Minimize host OS footprint (reduce attack surface) Reduce host attack surface seccomp which stands for secure computing was originally intended as a means of safely running untrusted compute-bound programs AppArmor can be configured for any application to reduce its potential host attack surface and provide greater in-depth defense. PSP enforces PodSecurityPolicy is deprecated as of Kubernetes v1.21, and will be removed in v1.25. We recommend migrating to Pod Security Admission. Apply host updates Install minimal required OS fingerprint Identify and address open ports Remove unnecessary packages Protect access to data with permissions Restirct allowed hostpaths 最小化IAM角色 Minimize IAM roles Access authentication and authorization 最小化对网络的外部访问 Minimize external access to the network if it means deny external traffic to outside the cluster?!! not tested, however, the thinking is that all pods can talk to all pods in all name spaces but not to the outside of the cluster!!! apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: deny-external-egress spec: podSelector: {} policyTypes: - Egress egress: to: - namespaceSelector: {} 适当使用内核强化工具，如AppArmor, seccomp Appropriately use kernel hardening tools such as AppArmor, seccomp AppArmor Seccomp Minimize Microservice Vulnerabilities - 20% # 设置适当的OS级安全域，例如使用PSP, OPA，安全上下文 Setup appropriate OS-level security domains e.g. using PSP, OPA, security contexts Pod Security Policies Open Policy Agent Security Contexts 管理Kubernetes机密 Manage kubernetes secrets 在多租户环境中使用容器运行时 (例如gvisor, kata容器) Use container runtime sandboxes in multi-tenant environments (e.g. gvisor, kata containers) 使用mTLS实现Pod对Pod加密 Implement pod to pod encryption by use of mTLS check if service mesh is part of the CKS exam Supply Chain Security - 20% # 最小化基本镜像大小 Minimize base image footprint\nMinimize base Image Use distroless, UBI minimal, Alpine, or relavent to your app nodejs, python but the minimal build. Do not include uncessary software not required for container during runtime e.g build tools and utilities, troubleshooting and debug binaries. Learnk8s: 3 simple tricks for smaller Docker images GKE 7 best practices for building containers 保护您的供应链：将允许的注册表列入白名单，对镜像进行签名和验证 Secure your supply chain: whitelist allowed image registries, sign and validate images\nUsing ImagePolicyWebhook admission Controller 使用用户工作负载的静态分析(例如kubernetes资源，Docker文件) Use static analysis of user workloads (e.g. kubernetes resources, docker files) 扫描镜像，找出已知的漏洞 Scan images for known vulnerabilities Aqua security Trivy Anchore command line scans Monitoring, Logging and Runtime Security - 20% # 在主机和容器级别执行系统调用进程和文件活动的行为分析，以检测恶意活动 Perform behavioural analytics of syscall process and file activities at the host and container level to detect malicious activities\nFalco installation guide Sysdig Falco 101 Falco Helm Chart Falco Kubernetes helmchart Detect CVE-2020-8557 using Falco 检测物理基础架构，应用程序，网络，数据，用户和工作负载中的威胁 Detect threats within a physical infrastructure, apps, networks, data, users and workloads\n检测攻击的所有阶段，无论它发生在哪里，如何扩散 Detect all phases of attack regardless where it occurs and how it spreads\nAttack Phases Kubernetes attack martix Microsoft blog MITRE attack framwork using Falco Lightboard video: Kubernetes attack matrix - 3 steps to mitigating the MITRE ATT\u0026amp;CK Techniques CNCF Webinar: Mitigating Kubernetes attacks 对环境中的不良行为者进行深入的分析调查和识别 Perform deep analytical investigation and identification of bad actors within the environment\nSysdig documentation Monitoring Kubernetes with sysdig CNCF Webinar: Getting started with container runtime security using Falco 确保容器在运行时不变 Ensure immutability of containers at runtime\n使用审计日志来监视访问 Use Audit Logs to monitor access\n"},{"id":22,"href":"/www6vCloudNative/ref/cloudNative/k8sRook/","title":"Kubernetes Rook","section":"Refs","content":"\n目录 # Rook for ceph # Rook建立Ceph集群流程[1] # rook的crd建立好之后，\nRook的operator会把rook的控制面建立出来\n然后把storageclass和csi driver都注册好。\n用户只要建立pvc，pvc里指定用哪个storageclass，然后csi driver会去工作。\nRook cluster installation [2][3] [I] # Ceph存储类型 # Block FileSystem Object Block Storage [4][5] [II] # install apiVersion: ceph.rook.io/v1 kind: CephBlockPool metadata: name: replicapool namespace: rook-ceph spec: failureDomain: host replicated: size: 3 --- apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: rook-ceph-block # Change \u0026#34;rook-ceph\u0026#34; provisioner prefix to match the operator namespace if needed provisioner: rook-ceph.rbd.csi.ceph.com parameters: # clusterID is the namespace where the rook cluster is running clusterID: rook-ceph # Ceph pool into which the RBD image shall be created pool: replicapool ... # Delete the rbd volume when a PVC is deleted reclaimPolicy: Delete Consume Block[II] eg 1. StatefulSet 通过StorageClass动态申请pv volumeClaimTemplates: -metadata: ... spec: ... storageClassName: rook-ceph-block volumeMode: Filesystem eg 2. pvc动态申请pv， deployment申请pvc\nvolumes: ... ... - name: pvc-test persistentVolumeClaim: claimName: rook-ceph-test-pvc FileSystem[6][7] [III] # PVC在线扩容，PVC快照和回滚 # 参考 # 云原生训练营 第0期-模块七\nCeph Cluster CRD ***\nCeph Storage Quickstart\nBlock Storage\nCeph Block Pool CRD\nShared Filesystem\nCeph Shared Filesystem CRD\nkubernetes架构师课程 杜宽 *** [I]P112-P114 installation, [II]P115-P117 block, [III] P118 FileSystem\n"},{"id":23,"href":"/www6vCloudNative/ref/cloudNative/k8sDNS/","title":"Kubernetes CoreDNS","section":"Refs","content":"\nCoreDNS插件架构 # github中\nKubernetes DNS # Kubernetes服务发现 自定义域名解析 # hosts: 存量代码域名调用 rewrite: 集群外服务调用 forward： 配置存根域 CoreDNS 部署架构 # kubesphere.local prod.kubesphere.local dev.kubesphere.local qa.kubesphere.local test.kubesphere.local reference: # 【Meetup 分享】使用 KubeSphere \u0026amp; CoreDNS 搭建云原生 DNS 详解 DNS 与 CoreDNS 的实现原理 未 "},{"id":24,"href":"/www6vCloudNative/ref/cloudNative/k8sPaaS/","title":"Kubernetes PaaS平台","section":"Refs","content":"\n/ OpenShift KubeSphere 网络 ovs(openshift SDN), flannel, calico calico, flannel, QinCloud CNI, Porter 存储 Ceph,NFS NeonSAN， QingCloud-csi，Ceph-csi CI/CD OCP Pipeline/Tekton\ns2i Pipeline, Jenkins, sonarqube\ns2i 微服务 istio istio ingress HA Proxy openELB infa(多云) kvm,openstack,aws,GCP,azure\nmarketplace OpenPitrix(aws, aliyun, openstack) Service Catelog operatorhub 无 serverless knative openfunction(基于knative) 多集群 solo、 kubefed(二开) 参考 # 青云官网\n"},{"id":25,"href":"/www6vCloudNative/ref/cloudNative/ceph/","title":"Ceph 总结","section":"Refs","content":"\n目录 # Ceph架构图 # 架构[2] # 基础存储系统\nrados:基础存现系统RADOS(Reliable,Autonomic,Distribuuted object store，既可靠的、自动化的、分布式的对象存储).所有存储在Ceph系统中的用户数据事实上最终都是由这一层来存储的。Ceph的高可靠、高可扩展、高性能、高自动化等等特性本质上也是由这一层所提供的。 RADOS -\nRADOS全称Reliable Autonomic Distrubuted object Store,是Ceph集群的精华，用户实现数据分配，Failover等集群操作 基础库librados:\nlibrodos:这一层的功能是对RADOS进行抽象和封装，并向上层提供API，以便直接基于RADOS进行应用开发。特别要注意的是，RADOS是一个对象存储系统，因此，librodos实现的API也只是针对对象存储功能的。 Libradio - Librados是RODOS提供库，因为RADOS是协议很难直接访问，因此上层的RBD、RGW和CephFS都是通过librados访问的，目前提供PHP、Ruby、Java、Python、C和C++支持。 高层应用接口\nradosgw:对象网关接口 rbd:块存储 cephfs：文件系统存储，其作用是在librodos库的基础上提供抽象层次更高、更便于应用或客户端使用的上层接口。 RBD -\nRBD全称RADOS Block Device，是ceph对外提供服务的块设备服务。 RGW -\nRGW全称RADOS gateway,是ceph对外提供的对象存储服务，接口与S3和Swift兼容 CephFS -\nCephFS全称Ceph File System，是ceph对外提供的文件系统服务。 Ceph主要有三个基本进程[2][5] # osd:\n用于集群中所有数据与对象的存储。处理集群数据的复制、恢复、回填、再负载。并向其他osd守护进程发送心跳，然后向Mon提供一些监控信息。\n当Ceph存储集群设定数据有两个副本时，则至少需要两个OSD守护进程即OSD节点，集群才能达到active+clean状态。\nMDS(可选)\n为Ceph文件系统提供元数据计算、缓存与同步(也就是说，ceph块设备和ceph对象存储不使用MDS)。在ceph中，元数据也是存储在osd节点中的，mds类似于元数据的代理缓存服务器。MDS进程并不是必须的进程，只有需要使用CEPHFS时，才需要配置MDS节点。\nMonitor\n监控整个集群的状态，维护集群的cluster MA二进制表，保证集群数据的一致性。ClusterMAP描述了对象存储的物理位置，以及一个将设备聚合到物理位置的桶列表。\nManager(ceph-mgr)\n用于收集ceph集群状态，运行指标，比如存储利用率、当前性能指标和系统负载。对外提供ceph dashboard(ceph-ui)和resetful api,manger组件开启高可用时，至少2个\nMDS -\nMDS全称Ceph Metadata Server，是CephFS服务依赖的元数据服务\nMonitor -\n监控整个集群的状态，维护集群的cluster MA二进制表，保证集群数据的一致性\nOSD -\nOSD全程Object storage Device,也就是负责响应客户端请求返回具体数据的进程。一个Ceph集群一般都有很多个OSD\nrbd: 不需要部署独立的守护进程 Cephfs 需要部署独立的守护进程 MDS 对象存储 需要部署独立的守护进程 radosgw 生产环境\nMonitor 需要至少3个 Manager 需要至少2个 Ceph核心组件及概念介绍[2][6] # Object\nCeph最底层的存储单元是Object对象，每个object包含数据和原始数据。\n[自带元数据]\n[id + binary data + metadate(key+value)] PG\nPG全称Placement Grouops,是一个逻辑的概念，一个PG包含多个OSD。引入PG这一层其实是为了更好的分配数据和定位数据。 CRUSH\nCRUSH是Ceph使用的数据分布算法，类似一致性哈希，让数据分配到预期的地方。 Ceph纠删码\n[数据保护, 数据恢复] 客户端的数据条带化\n存储内容进行顺序分片, 分布式存储每个分片 ceph支持的存储接口[7] # 部署 \u0026amp; 工具 # ceph ansible\n主流 rook operator 超融合模式\n混合部署 资源预留 存算分离模式 ceph独立部署, 打label 工具\nRedHat OCS sizing tool 场景[7] # openstack\n静态化 云原生 rook 动态化\n动态扩缩容 支持混合云，多云场景\n通过CSI接口，提供混合云环境下的一致性 拓扑感知\n副本分到到3个rack中\n跨AZ的多副本 multus管理网络， 提升性能[pic]\n管理网 - ovs\nceph private network - vlan10\nceph public network - vlan20 占比 Ceph RBD - 48% LVM - 15% NFS - 8% 参考 # 「基础理论」CEPH 基础介绍 good 分布式存储Ceph 分布式文件系统ceph知识整理 分布式存储 Ceph 的演进经验 · SOSP \u0026lsquo;19 马哥教育2021-Ceph分布式存储系统快速入门 《Ceph企业级分布式存储》 【直播回放】ROOK 云原生分布式存储开源项目的介绍及其在企业中的应用未来 "},{"id":26,"href":"/www6vCloudNative/ref/cloudNative/k8sOpenShift/","title":"Kubernetes OpenShift","section":"Refs","content":"\nInterface # CNI\nOpenFlow(OVS)(使用的， 更通用，场景适合的更多)\nCalico CRI OpenShift3.0 用的\nDocker（有守护进程，守护进程宕掉后，运行时会有问题）\nRed Hat Enterprise Linux - 4G大小 OpenShift4.0 用的\nCRI-O（没有守护进程）\nRet Hat CoreOS - 不到800M大小 CSI\nCEPH(ROOK)\nNFS 入口 ingress openshift 默认是haproxy的ingress（首选建议使用， dns解析， mTLS） 也可以装nginx ingress 也可以装istio ingress gateway（4层选， istio ingress gateway） PaaS # 角色 - RBAC\n配置管理 - LimitRanage， Quota\n联邦集群 - kubefed\n对K8S的增强 # Operator Multus多个虚拟网卡（用的多）\n一个pod有多个虚拟网卡， ovs管理网络 + macvlan网络（性能比较好） kube-virt 虚拟机管理 应用迁移 # 应用改造的难度比较大 多云 # 基于多云的CI/CD # 基于jenkins\n容易编写，耗内存 基于Tekton\n不容易编写， 不耗内存 应用发布到多个集群 # RHACM 本来是IBM的，后来给了RedHat\nRHACM基于CNCF的Open Cluster Management孵化项目\n竞品分析 # 占有率30%， 早2018年，\n生态做的好（开源项目operator， marketplace， istio， serverlesss）\nIBM AI，中间件相关的东西挪到OpenShift 市场 # 银行， 企业级客户\n社区版本 OKD\n参考 # 基于 Red Hat OpenShift 4 构建 Paas、DevOps 平台 v 15分钟搞定 OpenShift 多云管理，并统一部署应用 用 Kubernetes Operator 管理 OpenShift 应用生命周期 "},{"id":27,"href":"/www6vCloudNative/ref/cloudNative/k8sHA/","title":"K8S高可用-控制面","section":"Refs","content":"\n控制面 高可用 # 控制平面组件划分单独节点； 控制平面所在节点，应该确保在不同机架上； 保证控制平面的每个组件有足够的CPU、内存和磁盘资源； 减少或消除外部依赖； Cloud Provider API 核心组件以普通Pod形式加载运行时， 可能会调度到任意工作节点。 控制面 高可用方案 # [1] [2]\n架构图 # load balancer 虚ip\ncontroller-manager： 用lease来实现controller-manager和scheduler的leader election\n参考: # Options for Highly Available topology Creating Highly Available clusters with kubeadm "},{"id":28,"href":"/www6vCloudNative/ref/cloudNative/k8s-operator/","title":"Kubernetes Operator-kubebuilder","section":"Refs","content":"\n目录 # 使用kubebuilder构建Operator的过程 # 自己定义DaemonSet Operator包括： 在每个node上启动一个Pod（CRD + Controller） webhook(证书 + mutation + validation) Step1: init project # Create a kubebuilder project, which requires an empty folder # kubebuilder init --domain cncamp.io Check project layout # cat PROJECT domain: cncamp.io layout: - go.kubebuilder.io/v3 projectName: mysts repo: github.com/www6v/demo-operator version: \u0026#34;3\u0026#34; Step2: Create CRD and Controller # Create API, create resource[Y], create controller[Y] # kubebuilder create api --group apps --version v1beta1 --kind MyDaemonset edit api/v1alpha1/simplestatefulset_types.go # // MyDaemonsetSpec defines the desired state of MyDaemonset type MyDaemonsetSpec struct { // INSERT ADDITIONAL SPEC FIELDS - desired state of cluster // Important: Run \u0026#34;make\u0026#34; to regenerate code after modifying this file // Foo is an example field of MyDaemonset. Edit mydaemonset_types.go to remove/update Image string `json:\u0026#34;image,omitempty\u0026#34;` } // MyDaemonsetStatus defines the observed state of MyDaemonset type MyDaemonsetStatus struct { AvaiableReplicas int `json:\u0026#34;avaiableReplicas,omitempty\u0026#34;` // INSERT ADDITIONAL STATUS FIELD - define observed state of cluster // Important: Run \u0026#34;make\u0026#34; to regenerate code after modifying this file } Check Makefile # Build targets: ### create code skeletion manifests: generate crd generate: generate api functions, like deepCopy ### generate crd and install run: Run a controller from your host. install: Install CRDs into the K8s cluster specified in ~/.kube/config. ### docker build and deploy docker-build: Build docker image with the manager. docker-push: Push docker image with the manager. deploy: Deploy controller to the K8s cluster specified in ~/.kube/config. Edit controllers/mydaemonset_controller.go, add permissions to the controller # //+kubebuilder:rbac:groups=apps.cncamp.io,resources=mydaemonsets/finalizers,verbs=update // Add the following //+kubebuilder:rbac:groups=core,resources=nodes,verbs=get;list;watch //+kubebuilder:rbac:groups=core,resources=pods,verbs=get;list;watch;create;update;patch;delete Generate crd # make manifests Build \u0026amp; install # make build make docker-build make docker-push make deploy Step3: Enable webhooks # Install cert-manager # kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.6.1/cert-manager.yaml Create webhooks # kubebuilder create webhook --group apps --version v1beta1 --kind MyDaemonset --defaulting --programmatic-validation Change code # Enable webhook in config/default/kustomization.yaml # Redeploy # 附件: Operator源代码 # https://github.com/www6v/mydaemonset\n"},{"id":29,"href":"/www6vCloudNative/ref/cloudNative/k8sMultiTenancy/","title":"Kubernetes 多租户","section":"Refs","content":"\n一. 概念和原则 # 租户是指一组拥有访问特定软件资源权限的用户集合，在多租户环境中，它还包括共享的应用、服务、数据和各项配置等。 多租户集群必须将租户彼此隔离 集群须在租户之间公平的分配集群资源。 二. Overview # 软隔离：在这种情况下，我们有一个企业，不同的团队访问同一个集群，这需要较少的安全开销，因为用户可以相互信任。\n硬隔离：当 Kubernetes 暴露给具有独立且完全不受信任的用户的多个企业时，这是必需的。\n方案B 软隔离 - hierarchical-namespaces\nhttps://github.com/kubernetes-sigs/hierarchical-namespaces Kubernetes 的层级命名空间介绍\n方案C 硬隔离 - virtualcluster\nhttps://github.com/kubernetes-sigs/cluster-api-provider-nested/tree/main/virtualcluster\n三. 方案B 解决方案 # 1. 认证 # 识别访问的用户是谁 K8s可管理2类用户 ServiceAccount\n是存在于K8s中的虚拟账户;\n原生支持的动态认证; 用户认证\nwebhook是和企业认证平台集成的重要手段 2. 授权 # 3. 隔离 # 节点隔离\nTaints和Toleration机制\n[Kubernetes 调度 - 污点和容忍度详解] (https://mp.weixin.qq.com/s/rza4euQCLuMLTI5fHdj67Q)\nTaint 和 Toleration（污点和容忍）\n网路隔离 NetworkPolicy 网路策略 - 实质上也是iptables规则\n网络策略通过网络插件来实现，所以必须使用一种支持 NetworkPolicy 的网络方案（如 calico）—— 非 Controller 创建的资源，是不起作用的。\n容器隔离\n5种隔离机制\nMount Namespace 隔离 PID Namespace 隔离 Network Namespace 隔离 IPC Namespace 隔离 UTS Namespace 隔离 4个打破隔离的机制 - spec.hostNetwork\r- spec.volumes\r- spec.shareProcessNamespace\r- spec.containers[].securityContext.privileged 4. 配额 # ResourceQuota\n5. 开源方案 # kiosk\nSet up soft multi-tenancy with Kiosk on Amazon Elastic Kubernetes Service\n参考： # Kubernetes 多租户简介 https://static.sched.com/hosted_files/kccnceu19/74/kubecon-eu-multitenancy-wg-deepdive.pdf 官方 https://github.com/kubernetes-sigs/multi-tenancy\n解决 K8s 落地难题的方法论提炼\nNetworkPolicy jimmysong\n轻量级 Kubernetes 多租户方案的探索与实践 火山引擎云原生 本质上来说 KubeZoo 的方案和 Virtual Cluster 有点类似，是一种 Serverless 的 Kubernetes 方案。\nKubeSphere 多租户与认证鉴权实践：使用 GitLab 账号登陆 KubeSphere 未\n"},{"id":30,"href":"/www6vCloudNative/ref/cloudNative/k8sAbandonDocker/","title":"K8S 弃用Docker","section":"Refs","content":"\n1. # 如果以 kubelet 调用 CRI 为起点，OCI 的 runC 调用为终点，三种模式经历的可执行程序分别是：\ndockershim 模式：dockershim(*)-\u0026gt;dockd-\u0026gt;containerd-\u0026gt;containerd-shim\ncri-containerd 模式：cri-containerd(*)-\u0026gt; containerd-\u0026gt;containerd-shim\ncri-o 模式：cri-o\nK8S弃用Docker是指弃用Docker daemon\nRedHat 推崇的 cri-o → Podman\n2. # 容器引擎可选的替代方案大概有以下几种：\ncontainerd CRI-O → Podman, Red Hat gVisor → “guest 内核”层 kata-containers → 安全 Nabla 参考： # https://www.infoq.cn/article/VasFWOChD6JoL5avhfAz https://www.zhihu.com/question/433184969 "},{"id":31,"href":"/www6vCloudNative/ref/cloudNative/linuxKernelParam/","title":"虚拟机和容器中的内核参数 kernel","section":"Refs","content":"\n重要的内核参数 # ##内存与交换分区\rvm.swappiness\r# 内存分配 ## OOM\rvm.overcommit_memory=2: 过量使用. 0, 1, 2\rRAM, swap\rvm.overcommit_ratio=50\r比如：\rswap: 2G ，RAM: 8G\r则 memory = swap + RAM * ratio = 2G + 8G * 50% = 6G\r如何充分使用内存：\r1. swap跟RAM一样大； swappiness=0\r2. vm.overcommit_memory=2， vm.overcommit_ratio=100， swappiness=0 (生产中)\rmemory = swap + RAM * ratio ## 网络\rnet.ipv4.ip_forward # ipv4的路由转发功能\rnet.ipv4.tcp_tw_reuse = 1 #开启重用，允许将TIME_WAIT socket用于新的TCP连接。默认为0，表示关闭。\rnet.ipv4.tcp_tw_recycle = 1 #开启TCP连接中TIME_WAIT socket的快速回收。默认值为0，表示关闭\r。\rnet.ipv4.tcp_syncookies = 1 #开启SYN cookie，出现SYN等待队列溢出时启用cookie处理，防范少量的SYN攻击。默认为0，表示关闭。\rnet.ipv4.tcp_keepalive_time = 600 #keepalived启用时TCP发送keepalived消息的拼度。默认位2小时。\rnet.ipv4.tcp_keepalive_probes = 5 #TCP发送keepalive探测以确定该连接已经断开的次数。根据情形也可以适当地缩短此值。\rnet.ipv4.tcp_keepalive_intvl = 15 #探测消息发送的频率，乘以tcp_keepalive_probes就得到对于从开始探测以来没有响应的连接杀除的时间。\r默认值为75秒，也就是没有活动的连接将在大约11分钟以后将被丢弃。\r对于普通应用来说,这个值有一些偏大,可以根据需要改小.特别是web类服务器需要改小该值。\rnet.ipv4.ip_local_port_range = 1024 65000 #指定外部连接的端口范围。默认值为32768 61000。\rnet.ipv4.tcp_max_syn_backlog = 262144 #表示SYN队列的长度，预设为1024，这里设置队列长度为262 144，以容纳更多的等待连接。\rnet.core.somaxconn = 16384 #定义了系统中每一个端口最大的监听队列的长度, 对于一个经常处理新连接的高负载 web服务环境来说，默认值为128，偏小。\r## 缓冲区大小\rnet.ipv4.tcp_mem\rnet.ipv4.tcp_rmem\rnet.ipv4.tcp_wmem\r## core\rwmem_max\rwmem_default\rrmem_max\rrmem_default # IPC - 进程通讯\rmessage\rmsgmni\rmsgmax\rmsgmnb\rshm - sharememory\rshmall\rshmmax\rshmmni\rsemaphore # ls /proc/sys\rabi crypto debug dev fs kernel net vm\r# 多线程\rvm.max_map_count # 进程有若干操作系统资源的限制\r➜ ~ cat /proc/1/limits\rLimit Soft Limit Hard Limit Units\rMax cpu time unlimited unlimited seconds\rMax file size unlimited unlimited bytes\rMax data size unlimited unlimited bytes\rMax stack size 8388608 unlimited bytes\rMax core file size 0 unlimited bytes\rMax resident set unlimited unlimited bytes\rMax processes 15188 15188 processes\rMax open files 65536 65536 files\rMax locked memory 65536 65536 bytes\rMax address space unlimited unlimited bytes\rMax file locks unlimited unlimited locks\rMax pending signals 15188 15188 signals\rMax msgqueue size 819200 819200 bytes\rMax nice priority 0 0\rMax realtime priority 0 0\rMax realtime timeout unlimited unlimited us 容器中的内核参数 # # 已经namespace化的内核参数：\rkernel.shm*,\rkernel.msg*,\rkernel.sem,\rfs.mqueue.*,\rnet.*.\r# 没有namespace化\rvm.*\r# k8s把syctl参数分为safe和unsafe\r# 只有三个参数被认为是safe的\rkernel.shm_rmid_forced,\rnet.ipv4.ip_local_port_range,\rnet.ipv4.tcp_syncookies 参考 # 内核参数 # Linux内核常见参数的优化 单个JVM下支撑100w线程数vm.max_map_count Linux的overcommit配置 给容器设置内核参数 good TCP总结 self 06_虚拟化技术基础原理详解 V "},{"id":32,"href":"/www6vCloudNative/ref/cloudNative/k8sRuntime/","title":"Kubernetes Runtime","section":"Refs","content":"\n一. 虚拟化技术 # runc： OSContainerRuntime（基于进程隔离技术） kvm: HyperRuntime（基于Hypervisor技术） runv： UnikernelRuntime（基于unikernel） 二. CRI架构 # CRI架构\ndocker调用的链路：dockershim =\u0026gt; dockerd =\u0026gt; Containerd =\u0026gt; runc\nContainerd调用的链路：Containerd \u0026ndash;\u0026gt; shim v2 \u0026ndash;\u0026gt; runtimes\nDocker 作为 K8S 容器运行时，调用关系如下： kubelet \u0026ndash;\u0026gt; docker shim （在 kubelet 进程中） \u0026ndash;\u0026gt; dockerd \u0026ndash;\u0026gt; containerd\nContainerd 作为 K8S 容器运行时，调用关系如下： kubelet \u0026ndash;\u0026gt; cri plugin（在 containerd 进程中） \u0026ndash;\u0026gt; containerd\nhigh level运行时： Dockershim， containerd， CRI-O\nlow level运行时：\nrunc， kata， gVisor runc：运行可信容器（弱隔离但性能好）; runv: 运行不可信容器（强隔离安全性好）; 参考: # CRI - Container Runtime Interface（容器运行时接口） 为Kubernetes选择合适的容器运行时 模块七 腾讯云 "},{"id":33,"href":"/www6vCloudNative/ref/cloudNative/k8sOperator/","title":"Kubernetes Operator-Etcd","section":"Refs","content":"\nOperator: \u0026ldquo;有状态应用\u0026rdquo;, 自动化运维工作。\nEtcd Operator部署 # Controller + CRD etcd Controller Deployment etcd CRD API资源类型 kind: \u0026ldquo;EtcdCluster\u0026rdquo;\netcd高可靠： backup + restore\netcd backup etcd备份 etcd restore etcd恢复: 恢复备份的数据\n普通运维方式etcd运维步骤: # 创建种子节点（集群）的阶段称为：Bootstrap 通过 Etcd 命令行添加一个新成员： $ etcdctl member add infra1 http://10.0.1.11:2380 为这个成员节点生成对应的启动参数，并启动它： etcd\r--data-dir=/var/etcd/data\r--name=infra1\r--initial-advertise-peer-urls=http://10.0.1.11:2380\r--listen-peer-urls=http://0.0.0.0:2380\r--listen-client-urls=http://0.0.0.0:2379\r--advertise-client-urls=http://10.0.1.11:2379\r--initial-cluster=infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380\r--initial-cluster-state=existing etcd Operator # 注: 一个etcd集群，一个controller。\n当这个 YAML 文件第一次被提交到 Kubernetes 之后，Etcd Operator 的 Informer，就会立 刻“感知”到一个新的 EtcdCluster 对象被创建了出来。所以，EventHandler 里的“添加”事 件会被触发。\n而这个 Handler 要做的操作也很简单，即：在 Etcd Operator 内部创建一个对应的 Cluster 对 象（cluster.New），比如流程图里的 Cluster1。\n参考: # 深入剖析Kubernetes - 27 聪明的微创新：Operator工作原理解读 张磊 面向 Kubernetes 编程： Kubernetes 是下一代操作系统 awesome-operators obsolete operator-sdk 工具 %accordion%附件-有用的 Operator%accordion%\nelastic/cloud-on-k8s solo-io/envoy-operator coreos/etcd-operator lyft/flinkk8soperator *** banzaicloud/hpa-operator gianarb/influxdb-operator banzaicloud/istio-operator jaegertracing/jaeger-operator banzaicloud/kafka-operator *** oracle/mysql-operator Percona-Lab/percona-xtradb-cluster-operator coreos/prometheus-operator *** ucloud/redis-cluster-operator *** apache/rocketmq-operator*** rook/rook*** GoogleCloudPlatform/spark-on-k8s-operator *** pingcap/tidb-operator***\n%/accordion%\n"},{"id":34,"href":"/www6vCloudNative/ref/cloudNative/k8sAutoScale/","title":"Kubernetes自动伸缩和HPA","section":"Refs","content":"\n一. Core metrics(核心指标) # kubernetes的新监控体系中，metrics-server属于Core metrics(核心指标)，提供API metrics.k8s.io，仅提供Node和Pod的CPU和内存使用情况。而其他Custom Metrics(自定义指标)由Prometheus等组件来完成. 图14-3 14-5 14-6 基于Core metrics(核心指标)\nkube-aggregator # 有了Metrics Server组件，也采集到了该有的数据，也暴露了api，但因为api要统一，如何将请求到api-server的/apis/metrics请求转发给Metrics Server呢，解决方案就是：kube-aggregator,在k8s的1.7中已经完成，之前Metrics Server一直没有面世，就是耽误在了kube-aggregator这一步。\nkube-aggregator（聚合api）主要提供：\nProvide an API for registering API servers.\nSummarize discovery information from all the servers.\nProxy client requests to individual servers.\nMetrics Server # metric-server是扩展的apiserver.\nMetrics Server: metric-aggregator, 聚合metric。\nMetrics Server 是集群级别的资源利用率数据的聚合器（ aggregator ）。Metrics Server 通过Kubernetes 聚合器（ kube-aggregator）注册到主API Server 之上，而后基于kubelet 的Summary API 收集每个节点上的指标数据，并将它们存储于内存中然后以指标API 格式提供。\nmetric api # metric api的使用：\nMetrics API 只可以查询当前的度量数据，并不保存历史数据\nMetrics API URI 为 /apis/metrics.k8s.io/，在 k8s.io/metrics 维护\n必须部署 metrics-server 才能使用该 API，metrics-server 通过调用 Kubelet Summary API 获取数据\n二. 自定义指标（Custom Metrics）与Prometheus # Kubernetes 里的 Custom Metrics 机制，是借助 Aggregator APIServer 扩展机制来实现的。这里的具体原理是，当你把 Custom Metrics APIServer 启动之后，Kubernetes里就会出现一个叫作custom.metrics.k8s.io的 API。而当你访问这个 URL 时，Aggregator 就会把你的请求转发给 Custom Metrics APIServer 。而 Custom Metrics APIServer 的实现，其实就是一个 Prometheus 项目的 Adaptor。\n目前Kubernetes中自定义指标一般由Prometheus来提供，再利用k8s-prometheus-adapter聚合到apiserver，实现和核心指标（metric-server)同样的效果\nPrometheus可以采集其它各种指标，但是prometheus采集到的metrics并不能直接给k8s用，因为两者数据格式不兼容，因此还需要另外一个组件(kube-state-metrics)，将prometheus的metrics数据格式转换成k8s API接口能识别的格式，转换以后，因为是自定义API，所以还需要用Kubernetes aggregator在主API服务器中注册，以便直接通过/apis/来访问。\n文件清单：\nnode-exporter：prometheus的export，收集Node级别的监控数据\nprometheus：监控服务端，从node-exporter拉数据并存储为时序数据。\nkube-state-metrics：将prometheus中可以用PromQL查询到的指标数据转换成k8s对应的数\nk8s-prometheus-adapter：聚合进apiserver，即一种custom-metrics-apiserver实现\n开启Kubernetes aggregator功能（参考上文metric-server）\n三. HPA # 四. 实战 # HPA 基于http_requests custom metrics 容器监控实践—Custom Metrics 参考: # 《Kubernetes进阶实战》 马永亮 《深入剖析Kubernetes - 49 Custom Metrics 让Auto Scaling不再“食之无味”》 张磊 container-monitor git metrics-server "},{"id":35,"href":"/www6vCloudNative/ref/cloudNative/k8sRBAC/","title":"Kubenetes RBAC","section":"Refs","content":"\n参考 # 《深入剖析Kubernetes - 26 基于角色的权限控制：RBAC》 张磊 "},{"id":36,"href":"/www6vCloudNative/ref/cloudNative/k8sService/","title":"Kubernetes服务","section":"Refs","content":"\n一. Kubenetes服务 # Kubernetes服务发现架构\nClusterIP 模式的 Service: 稳定的 IP 地址，即 VIP. ClusterIP是VIP, 虚拟IP. Headless Service: 稳定的 DNS 名字, 名字是通过 Pod 名字和 Service 名字拼接出来. 1.1 服务对外暴露方式 # NodePort 四层 ![node-port](https://user-images.githubusercontent.com/5608425/68234082-7d17be80-003b-11ea-891f-90a9e174bbc8.png)\r一定要对流出的包做 SNAT操作[2][6] client\r\\ ^\r\\ \\\rv \\\rnode 1 \u0026lt;--- node 2\r| ^ SNAT\r| | ---\u0026gt;\rv |\rendpoint 获取真实客户端IP, 设置Service 的 spec.externalTrafficPolicy 字段设置为 local，[2][6] client\r^ / \\\r/ / \\\r/ v X\rnode 1 node 2\r^ |\r| |\r| v\rendpoint Service LoadBalancer 四层 ![loadbalancer](https://user-images.githubusercontent.com/5608425/68290997-e216f700-00c3-11ea-82d3-b5e3f4c565a1.jpg)\rLoadBalancer类型的Service被提交后，Kubernetes就会调用CloudProvider[5]在公有云上为你创建一个负载均衡服务，并且把被代理的 Pod 的 IP地址配置给负载均衡服务做后端.\nExternalName ExternalName 类型的 Service，其实是在 kubedns里为你添加了一条 CNAME 记录. 1.7 之后支持的一个新特性.\nIngress Controller 七层\n![ingress-1](https://user-images.githubusercontent.com/5608425/68234079-7c7f2800-003b-11ea-8ada-2c034db8b25a.png)\r![ingress-2](https://user-images.githubusercontent.com/5608425/68234081-7c7f2800-003b-11ea-804c-1c5d87164d06.png) Ingress 服务: 全局的、为了代理不同后端 Service 而设置的负载均衡服务. Ingress 对象，其实就是 Kubernetes 项目对“反向代理”的一种抽象。 Ingress Controller: Nginx、HAProxy、Envoy、Traefik Ingress controller\n为了使 Ingress 正常工作，集群中必须运行 Ingress controller。 这与其他类型的控制器不同，其他类型的控制器通常作为 kube-controller-manager 二进制文件的一部分运行，在集群启动时自动启动。 你需要选择最适合自己集群的 Ingress controller 或者自己实现一个。\nKubernetes 当前支持并维护 GCE 和 nginx 两种 controller F5（公司）支持并维护 F5 BIG-IP Controller for Kubernetes Kong 同时支持并维护 社区版 与 企业版 的 Kong Ingress Controller for Kubernetes Traefik 是功能齐全的 ingress controller（Let’s Encrypt, secrets, http2, websocket…）, Containous 也对其提供商业支持。 Istio 使用 CRD Gateway 来 控制 Ingress 流量。 1.2 通过DNS发现服务 # 每个Service对象相关的DNS记录有两个： {SVCNAME}.{NAMESPACE}.{CLUSTER_DOMAIN} {SVCNAME}.{NAMESPACE}.svc.{CLUSTER_DOMAIN}\nroot@kubia-9nvx7:/# cat /etc/resolv.conf\rnameserver 172.17.0.2\rsearch default.svc.cluster.local svc.cluster.local cluster.local\roptions ndots:5 （1）拥有ClusterIP的Service资源，要具有以下类型的资源记录\rA记录：\u0026lt;service\u0026gt;.\u0026lt;ns\u0026gt;.svc.\u0026lt;zone\u0026gt;. \u0026lt;ttl\u0026gt; IN A \u0026lt;cluster-ip\u0026gt;\r（2）Headless类型的Service资源，要具有以下类型的资源记录\rA记录：\u0026lt;service\u0026gt;.\u0026lt;ns\u0026gt;.svc.\u0026lt;zone\u0026gt;. \u0026lt;ttl\u0026gt; IN A \u0026lt;endpoint-ip\u0026gt;\r（3）ExternalName类型的Service资源，要具有CNAME类型的资源记录\rCNAME记录：\u0026lt;service\u0026gt;.\u0026lt;ns\u0026gt;.svc.\u0026lt;zone\u0026gt;. \u0026lt;ttl\u0026gt; IN CNAME \u0026lt;extname\u0026gt;. 1.3 ClusterIP模式的yaml配置 # Service（接口声明） + Deployment（ endpoint 接口实现）\n二. Kubenetes服务工作原理 # Service是由kube-proxy组件，加上iptables来共同实现的。 kube-proxy的作用: 网络配置 2.1 kube-proxy # userspace 代理模式 # userspace 代理模式\riptables Proxy Mode # Iptables Proxy Mode\r-A OUTPUT -m comment --comment \u0026#34;kubernetes service portals\u0026#34; -j KUBE-SERVICES\r# 访问10.107.54.95后跳转到KUBE-SVC-4N57TFCL4MD7ZTDA链\r-A KUBE-SERVICES -d 10.107.54.95/32 -p tcp -m comment --comment \u0026#34;default/nginx: cluster IP\u0026#34; -m tcp --dport 80 -j KUBE-SVC-4N57TFCL4MD7ZTDA\r# 随机转发的目的地，分别是 KUBE-SEP-UZXILYFQQ2IZUWN5 和 KUBE-SEP-43IWXJI557JKCKCF\r-N KUBE-SVC-4N57TFCL4MD7ZTDA\r-A KUBE-SVC-4N57TFCL4MD7ZTDA -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-UZXILYFQQ2IZUWN5\r-A KUBE-SVC-4N57TFCL4MD7ZTDA -j KUBE-SEP-43IWXJI557JKCKCF\r## DNAT到pod的ip和端口\r-A KUBE-SEP-UZXILYFQQ2IZUWN5 -p tcp -m tcp -j DNAT --to-destination 172.17.0.4:80\r-A KUBE-SEP-43IWXJI557JKCKCF -p tcp -m tcp -j DNAT --to-destination 172.17.0.5:80 IPVS proxy mode # IPVS是LVS一个组件，提供高性能、高可靠性的四层负载均衡器。IPVS 是IP Virtual Server的简写。IPVS构建在netfilter上，作为Linux 内核的一部分，从传输层实现了负载均衡。\n与iptables类似，ipvs基于netfilter 的 hook 功能，但使用哈希表作为底层数据结构并在内核空间中工作。这意味着ipvs可以更快地重定向流量，并且在同步代理规则时具有更好的性能。此外，ipvs为负载均衡算法提供了更多选项，例如：\nrr：轮询调度\rlc：最小连接数\rdh：目标哈希\rsh：源哈希\rsed：最短期望延迟\rnq： 不排队调度\r参考: # 深入理解 Kubernetes之：Service good 第14 章 ： Kubernetes Services 阿里 《Kubenetes in Action》 七牛容器云团队 Kubernetes中的服务发现机制与方式 马永亮 从 K8S 的 Cloud Provider 到 CCM 的演进之路 毛宏斌 百度 获取真实客户端IP 华为云在 K8S 大规模场景下的 Service 性能优化实践 未 Service jimmysong Ingress jimmysong 《深入剖析Kubernetes》 张磊\n《37 找到容器不容易：Service、DNS与服务发现》 《38 从外界连通Service与Service调试“三板斧”》 《39 谈谈Service与Ingress》 "},{"id":37,"href":"/www6vCloudNative/ref/cloudNative/k8sStorage/","title":"Kubernetes存储","section":"Refs","content":"\n![Kubernetes存储](.\\k8sStorage\\k8sStorage.jpg)\r一. Kubernetes 存储的绑定流程 # 1.1 # 三个阶段\n第一个create阶段，主要是创建存储； 第二个attach阶段，就是将那块存储挂载到 node 上面(通常为将存储load到node的/dev下面)； 第三个mount阶段，将对应的存储进一步挂载到 pod 可以使用的路径。 二. Static Provisioning \u0026amp;\u0026amp; Dynamic Provisioning # 2.1 Static Provisioning # 2.2 Dynamic Provisioning # ![relationship](https://user-images.githubusercontent.com/5608425/64247540-aafc5c00-cf41-11e9-83af-64199e79ded7.JPG)\r**Kubernetes pvc 动态绑定流程**\r只有同属于一个 StorageClass 的PV 和 PVC，才可以绑定在一起\nCSI driver动态创建pv ， CSI Driver 可以动态创建PV [5]\n创建pvc之后，provisioner 会创建pv ，并做pvc和pv的绑定关系 本地存储[5] # emptyDir\n不是overlayFS ，和写主机的性能是一样的\n和容器的生命周期一致，如果容器销毁， emptyDir也被销毁。 hostPath\n需要的注意点，不建议用 最佳实践[5] # 用户去查询集群中有哪些storageclass\n可能有本地的hostpath，或者远程的nfs， ceph 等等 在storageClass中, provisioner很重要 pod中声明一个pvc\ncsi plugin把pv attach到对应的node，mount到对应的pod pvc和pv是一一对应的关系 参考: # 《Kubenetes in Action》七牛容器云团队 \u0026laquo;深入剖析Kubernetes - 28 PV、PVC、StorageClass，这些到底在说啥？\u0026raquo; 张磊 \u0026laquo;深入剖析Kubernetes - 29 PV、PVC体系是不是多此一举？从本地持久化卷谈起\u0026raquo; 张磊 第9 章 ： 应用存储和持久化数据卷：核心知识 云原生训练营 第0期-模块七 "},{"id":38,"href":"/www6vCloudNative/ref/cloudNative/k8sDeclarativeAPI/","title":"Kubernetes声明式API","section":"Refs","content":"\n一. CRD # # CRD的定义和使用\r$ tree\r.\r├── controller.go + 自定义控制器： 拿到“实际状态”，然后拿它去跟“期望状态”做对比，执行“业务逻辑”\r├── crd\r│ └── network.yaml + resource的定义【像类定义】\r├── example\r│ └── example-network.yaml + resource的使用【像类实例】\r├── main.go\r└── pkg\r├── apis\r│ └── samplecrd + group │ ├── constants.go\r│ └── v1 + version\r│ ├── doc.go + 代码生成，Global Tag │ ├── register.go + 注册类型（Type）到APIServer中，全局变量\r│ ├── types.go + 类型有哪些字段\r│ └── zz_generated.deepcopy.go 【codegenerated】\r└── client +for自定义控制器【codegenerated】： 拿到“期望状态” ├── clientset\r├── informers + 见下图\r└── listers + 见下图 二. 自定义控制器 # 自定义控制器\n+Informer就是一个自带缓存和索引机制，可以触发 Handler 的客户端库。这个本地缓存在 Kubernetes 中一般被称为 Store，索引一般被称为 Index。 +Informer 使用了 Reflector 包，它是一个可以通过 ListAndWatch 机制获取并监视 API 对象变化的客户端封装。 +Reflector 和 Informer 之间，用到了一个“增量先进先出队列”进行协同。而 Informer 与你要编写的控制循环之间，则使用了一个工作队列来进行协同。\n+在实际应用中，除了控制循环之外的所有代码，实际上都是 Kubernetes 为你自动生成的，即：pkg/client/{informers, listers, clientset}里的内容。 +这些自动生成的代码，就为我们提供了一个可靠而高效地获取 API 对象“期望状态”的编程库。 +所以，接下来，作为开发者，你就只需要关注如何拿到“实际状态”，然后如何拿它去跟“期望状态”做对比，从而决定接下来要做的业务逻辑即可。 /// controller.go\n参考: # 《深入剖析Kubernetes - 23 声明式API与Kubernetes编程范式》 张磊 《深入剖析Kubernetes - 24 深入解析声明式API（一）：API对象的奥秘》 张磊 《深入剖析Kubernetes - 25 深入解析声明式API（二）：编写自定义控制器》 张磊 Kubernetes 准入控制 Admission Controller 介绍 CRD 代码示例 k8s自定义controller三部曲之一:创建CRD（Custom Resource Definition） 一、二、三 "},{"id":39,"href":"/www6vCloudNative/ref/cloudNative/k8sNetwork/","title":"Kubernetes网络","section":"Refs","content":"\n一. 容器和容器之间的网络 # 使用Docker的一种网络模型：–net=container 每个Pod容器有有一个pause容器 Pause容器 例子 Kubernetes之Pause容器 Kubernetes之“暂停”容器 二. Pod与Pod之间的网络 # 2.0 Overview # 2.1 同节点pod通信 # 基础: 网桥 bridge\n通过网桥通信\n图2. 同节点pod通信\n2.2 不同节点中的Pod通信（跨主机网络通讯） # 2.2.1 Overlay (Flannel方案) # flannel-UDP模式(三层overlay) 原理： fannelId进程封装/解开虚拟网卡docker0,fannel0的数据; 三层的overlay网络; 组件： TUN设备是3层的虚拟网络设备 ; fannel0 劣势: 三次用户态和内核态切换 ; 性能差， 已弃用 图3. flannel-UDP模式\n图4. flannel-UDP模式\nflannel-vxlan模式(两层虚拟网络) VXLAN 的覆盖网络的设计思想是：在现有的三层网络之上，“覆盖”一层虚拟的、由内核 VXLAN 模块负责维护的二层网络，使得连接在这个 VXLAN 二层网络上的“主机”（虚拟机或者容器都可 以）之间，可以像在同一个局域网（LAN）里那样自由通信 组件： VTEP（VXLAN Tunnel End Point）设备; fannel.1; 组成一个虚拟的两层网络 优势： 进行封装和解封装的对象，是二层数据帧（Ethernet frame）; 而且这个工作的执行流程，全部是在内核里完成的（因为VXLAN本身就是内核中的一个模块）; 主流的网络容器方案。 图5. flannel-vxlan模式\n图6. flannel-vxlan模式\n2.2.2 纯3层网络方案 # Flannel host-gw模式\nhost-gw 模式工作原理： 其实就是将每个 Flannel 子网（Flannel Subnet，比如：10.244.1.0/24）的“下一跳”，设置成了该子网对应的宿主机的 IP 地址。 也就是说，这台“主机”（Host）会充当这条容器通信路径里的“网关”（Gateway）。这也正是“host-gw”的含义。\n核心 路由规则： \u0026lt;目的容器 IP 地址段\u0026gt; via \u0026lt;网关的 IP 地址\u0026gt; dev eth0\n$ ip route\r...\r10.244.1.0/24 via 10.168.0.3 dev eth0 优势： 根据实际的测试，host-gw 的性能损失大约在 10% 左右，而其他所有基于 VXLAN“隧道”机制的网络方案，性能损失都在 20%~30% 左右。 Calico\n原理:\n基于iptable/linux kernel包转发; 根据iptables规则进行路由转发; 非overlay, Calico 没有使用 CNI 的网桥模式; 核心\n路由规则： \u0026lt;目的容器 IP 地址段\u0026gt; via \u0026lt;网关的 IP 地址\u0026gt; dev eth0 组件:\n路由规则; iptables的配置组件Felix; 路由广播组件BGP Speaker; Host Network模式\n容器的网络和宿主机的网络打平，在同一层; underlay方案; 2.2.3 总结: # flannel-UDP模式和flannel-vxlan模式都可以称作\u0026quot;隧道\u0026quot;机制；都是是overlay的。 普适性最强 flannel-VxLan (OpenShift使用) 二层可直连可选用Calico / Flannel host-gw / overlay（隧道） underlay（路由） L2 Vxlan(大二层 通讯双方在同一逻辑网段内) IPvlan L2模式\nMacvlan 9 L3 flannel-UDP（类似L2 overlay， 在节点上增加一个网关） IPvlan L3模式\nflannel host-gw（不能跨两层网络） Calico BGP组网方式（大三层） 三. Pod与Service之间的网络 # Kubernetes服务\n四. Internet与Service之间的网络 # 4.1 Service到Internet # iptables执行源NAT( SNAT )\n4.2 Internet到Service # Kubernetes服务\n五. 网络隔离 多租户 # NetworkPolicy\n六. 访问公网[99] # 通过NAT访问公网 # 参考: # 1.第13 章 ： Kubernetes网络概念及策略控制 CNCF × Alibaba 云原生技术公开课\n2.calico网络原理及与flannel对比\n3.Kubernetes CNI网络最强对比：Flannel、Calico、Canal和Weave\n8.K8s网络模型 阿里 加多 ***\n9.macvlan和ipvlan介绍及在k8s中的使用\n趣谈网络协议 刘超\n《30容器网络之Flannel：每人一亩三分地》 《31容器网络之Calico：为高效说出善意的谎言》 深入剖析Kubernetes 张磊\n《32 浅谈容器网络》 《33 深入解析容器跨主机网络》 《34 Kubernetes网络模型与CNI网络插件》 《35 解读Kubernetes三层网络方案》 K8S在UCloud内部的应用-高鹏 "},{"id":40,"href":"/www6vCloudNative/ref/cloudNative/k8sScheduler/","title":"Kubernetes调度器","section":"Refs","content":"\n资源调度泛型 [1] # ![调度系统泛型](https://user-images.githubusercontent.com/5608425/65023010-96b65700-d964-11e9-9acd-7cc8edbbde85.JPG)\r调度系统泛型\r类型 资源选择 排他性 分配粒度 集群策略 中央调度器 全局 无，时序 全局策略 严格的优先级(抢占式) 两层调度调度器 动态资源集 悲观锁 增量囤积 严格公正 共享状态 全局 乐观锁 调度器策略 优先级抢占 表1. 常见调度器的比较\nKubernetes 资源调度[2] # Kubernetes 调度的两个阶段[4][5] # 基于谓词和优先级的调度器（Predicates and Priorities） · v1.0.0 ~ v1.14.0 # 调度器扩展（Scheduler Extender） · v1.2.0 - Scheduler extension\n通过调用外部调度器扩展的方式改变调度器的决策；\nMap-Reduce 优先级算法 · v1.5.0 - MapReduce-like scheduler priority functions\n为调度器的优先级算法支持 Map-Reduce 的计算方式，通过引入可并行的 Map 阶段优化调度器的计算性能；\n基于调度框架的调度器（Scheduling Framework） · v1.15.0 ~ 至今 # 调度框架认为 Kubernetes 中目前存在调度（Scheduling）和绑定（Binding）两个循环：\n调度循环在多个 Node 中为 Pod 选择最合适的 Node；\n绑定循环将调度决策应用到集群中，包括绑定 Pod 和 Node、绑定持久存储等工作；\n除了两个大循环之外，调度框架中还包含 QueueSort、PreFilter、Filter、PostFilter、Score、Reserve、Permit、PreBind、Bind、PostBind 和 Unreserve 11 个扩展点（Extension Point），这些扩展点会在调度的过程中触发。\n批量计算[3] # K8s自带的的资源调度器，有一个明显的特点是：依次调度每个容器。\nVolcano\nDRF（dominant resource fairness）: Yarn和Mesos都有\nDRF意为：“谁要的资源少，谁的优先级高” Queue: Yarn调度器的功能 实战[11] # 服务资源智能推算: crane+Victoria Metrics Crane 调度器 [14]\ncrane-sheduler 基于prometheus集群真实资源负载进行调度，将其应用于调度过程中的 Filter 和 Score 阶段，能够有效缓解集群资源负载不均的问题，真正实现企业的降本增效。 二次调度: descheduler [12][13] 弹性调度： OpenKruise-WorkloadSpread + Virtual Kubelet 参考 # 《大数据日知录：架构与算法 第4章 张俊林 《Kubenetes in Action》 第11章-机理 第16章-高级调度 七牛容器云团队 为什么K8s需要Volcano？ 华为 scheduling framework调度器 \u0026amp;\u0026amp; 谓词 # 调度系统设计精要 linux 调度器， go调度器， k8s调度器 Kubernetes Scheduler 设计与实现 bili 第 76 期 Kubernetes Scheduler 设计与实现 *** https://github.com/kubernetes/enhancements/issues/895 even pod, 多个region调度 进击的 Kubernetes 调度系统（一）：Kubernetes scheduling framework 未 进击的 Kubernetes 调度系统（二）：支持批任务的 Coscheduling/Gang scheduling 未 基于谓词的调度器 # Kubernetes集群调度器原理剖析及思考 - v1.11版本 2019 深度解析Kubernetes核心原理之Scheduler 未 - KubeCon 2018 *** DockOne微信分享（一四九）：Kubernetes调度详解 FreeWheel 主任工程师-2017年-***未 实战 # 容器云调度优化及实践 V descheduler 二次调度让 Kubernetes 负载更均衡 Kubernetes 中 Descheduler 组件的使用与扩展 Crane 调度器介绍——一款在 Kubernetes 集群间迁移应用的调度插件 【腾讯云Finops Crane集训营】降本增效神器Crane实战记录 "},{"id":41,"href":"/www6vCloudNative/ref/SUMMARY/","title":"Summary","section":"Refs","content":" Part I # 云原生 序 Part II # 编排原理\nKubernetes Deployment Kubernetes Workload Kubernetes StatefulSet原理和源码 Kubenetes 资源模型 kubelet和PLEG Operator \u0026amp;\u0026amp; Controller\nKubernetes Operator-kubebuilder Kubernetes Operator-Etcd Kubernetes声明式API Kubernetes Operator-Redis K8s AdmissionWebhook Container Runtime\nKubernetes Runtime K8S 弃用Docker 网络\nKubernetes网络 Calico Calico 服务和DNS\nKubernetes服务 Kubernetes CoreDNS Kubernetes Nginx Ingress 存储\nKubernetes存储 Kubernetes Rook Ceph 总结 etcd 总结 调度\nKubernetes调度器 Kubernetes 高级调度 监控和AutoScale\nKubernetes自动伸缩和HPA 可观测性-Prometheus 可观测性-Prometheus HA 可观测性-Kubernetes 生产化\nAvailable K8S高可用-控制面 K8S高可用-零停机[自主中断] K8S高可用-零停机[探针] 集成 K8S 应用迁移至K8S K8s 集成Springcloud 虚拟机和容器中的内核参数 kernel Kubernetes 升级upgrade PaaS\nKubernetes PaaS平台 Kubernetes OpenShift Kubernetes 多租户 Kubenetes RBAC Kubernetes 多集群管理 Kubernetes和VM 安全\nKubernetes安全-Security Kubernetes CKS Kubernetes 安全实践 "}]